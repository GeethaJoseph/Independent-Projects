{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import reciprocal\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.8.0'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.8.0'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_mnist = keras.datasets.fashion_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_full.shape #When loading MNIST or Fashion MNIST using Keras rather than Scikit.Learn,every image is represented as a 28 Ã— 28 array rather than a 1D array of size 784"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('uint8')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_full.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_full.shape[-2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid, X_train = X_train_full[:5000]/255.0, X_train_full[5000:]/255.0\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9, 0, 0, ..., 5, 8, 3], dtype=uint8)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n",
    "\"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Coat'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names[y_train[0]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the model using the Sequential API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[28,28]))\n",
    "model.add(keras.layers.Dense(300, activation=\"relu\"))\n",
    "model.add(keras.layers.Dense(100, activation=\"relu\"))\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 =keras.models.Sequential([keras.layers.Flatten(input_shape=X_train_full.shape[-2:]),keras.layers.Dense(300, activation=\"relu\"), keras.layers.Dense(300, activation=\"relu\"),keras.layers.Dense(10,activation=\"softmax\")] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_6 (Flatten)         (None, 784)               0         \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 300)               235500    \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 100)               30100     \n",
      "                                                                 \n",
      " dense_20 (Dense)            (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 266,610\n",
      "Trainable params: 266,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.layers.core.flatten.Flatten at 0x1d774ede448>,\n",
       " <keras.layers.core.dense.Dense at 0x1d774ecef48>,\n",
       " <keras.layers.core.dense.Dense at 0x1d775760c88>,\n",
       " <keras.layers.core.dense.Dense at 0x1d74e0aa988>]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dense_18'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden1 = model.layers[1]\n",
    "hidden2 = model.layers[2]\n",
    "hidden3 = model.layers[2]\n",
    "hidden1.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#All the parameters of a layer can be accessed using its get_weights() and set_weights() methods.\n",
    "\n",
    "weights1, biases1 = hidden1.get_weights()\n",
    "weights2, biases2 = hidden2.get_weights()\n",
    "weights3, biases3 = hidden3.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 300)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.array(weights1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 100)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(weights2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 100)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(weights3).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compiling the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.compile(loss=\"sparse_categorical_crossentropy\", optimizer='sgd', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Evaluating the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.7082 - accuracy: 0.7663 - val_loss: 0.4971 - val_accuracy: 0.8354\n",
      "Epoch 2/10\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.4881 - accuracy: 0.8304 - val_loss: 0.4629 - val_accuracy: 0.8412\n",
      "Epoch 3/10\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.4428 - accuracy: 0.8449 - val_loss: 0.4308 - val_accuracy: 0.8512\n",
      "Epoch 4/10\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.4146 - accuracy: 0.8549 - val_loss: 0.4407 - val_accuracy: 0.8356\n",
      "Epoch 5/10\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.3948 - accuracy: 0.8609 - val_loss: 0.3825 - val_accuracy: 0.8700\n",
      "Epoch 6/10\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.3795 - accuracy: 0.8669 - val_loss: 0.3752 - val_accuracy: 0.8700\n",
      "Epoch 7/10\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.3655 - accuracy: 0.8705 - val_loss: 0.3560 - val_accuracy: 0.8768\n",
      "Epoch 8/10\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.3546 - accuracy: 0.8733 - val_loss: 0.3578 - val_accuracy: 0.8730\n",
      "Epoch 9/10\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.3429 - accuracy: 0.8778 - val_loss: 0.3507 - val_accuracy: 0.8772\n",
      "Epoch 10/10\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.3337 - accuracy: 0.8812 - val_loss: 0.3467 - val_accuracy: 0.8740\n"
     ]
    }
   ],
   "source": [
    "history = model1.fit(X_train, y_train, epochs=10, validation_data = (X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'verbose': 1, 'epochs': 10, 'steps': 1719}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "pd.DataFrame(history.history).plot(figsize=(8,5))\n",
    "plt.grid = True\n",
    "plt.gca().set_ylim(0,1)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 3ms/step - loss: 60.5066 - accuracy: 0.8352\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[60.506587982177734, 0.8352000117301941]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using model to make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = X_test[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_probablities = model.predict(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.9527379e-08, 8.3542354e-22, 0.0000000e+00, 2.5633831e-19,\n",
       "        1.0000000e+00, 2.8606927e-37, 0.0000000e+00, 2.1262550e-26,\n",
       "        0.0000000e+00, 0.0000000e+00],\n",
       "       [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        3.0239779e-11, 0.0000000e+00, 0.0000000e+00, 1.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00],\n",
       "       [9.2832547e-19, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        1.0000000e+00, 0.0000000e+00, 0.0000000e+00, 4.8542397e-31,\n",
       "        0.0000000e+00, 0.0000000e+00]], dtype=float32)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_probablities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_probablities.round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.argmax(y_probablities, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6, 5, 5], dtype=int64)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T-shirt/top',\n",
       " 'Trouser',\n",
       " 'Pullover',\n",
       " 'Dress',\n",
       " 'Coat',\n",
       " 'Sandal',\n",
       " 'Shirt',\n",
       " 'Sneaker',\n",
       " 'Bag',\n",
       " 'Ankle boot']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Shirt', 'Sandal', 'Sandal'], dtype='<U11')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " np.array(class_names)[y_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9, 2, 1], dtype=uint8)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_new = y_test[:3]\n",
    "y_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a Regression MLP Using the Sequential API "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = fetch_california_housing()\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data, housing.target)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train =scaler.fit_transform(X_train)\n",
    "X_valid = scaler.transform(X_valid)\n",
    "X_test =scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_train.shape[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape = X_train.shape[1:]))\n",
    "model.add(keras.layers.Dense(30,activation=\"relu\"))\n",
    "model.add(keras.layers.Dense(1))          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "keras.layers.Dense(30, activation=\"relu\",\n",
    "input_shape=X_train.shape[1:]),\n",
    "keras.layers.Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = \"mean_squared_error\", optimizer=keras.optimizers.SGD(learning_rate=1e-3), metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 2.5181 - accuracy: 0.0022 - val_loss: 0.9514 - val_accuracy: 0.0026\n",
      "Epoch 2/10\n",
      "363/363 [==============================] - 0s 839us/step - loss: 0.8786 - accuracy: 0.0028 - val_loss: 0.7321 - val_accuracy: 0.0026\n",
      "Epoch 3/10\n",
      "363/363 [==============================] - 0s 896us/step - loss: 0.7049 - accuracy: 0.0028 - val_loss: 0.6974 - val_accuracy: 0.0026\n",
      "Epoch 4/10\n",
      "363/363 [==============================] - 0s 828us/step - loss: 0.6668 - accuracy: 0.0028 - val_loss: 0.6780 - val_accuracy: 0.0026\n",
      "Epoch 5/10\n",
      "363/363 [==============================] - 0s 845us/step - loss: 0.6453 - accuracy: 0.0028 - val_loss: 0.6596 - val_accuracy: 0.0026\n",
      "Epoch 6/10\n",
      "363/363 [==============================] - 0s 819us/step - loss: 0.6278 - accuracy: 0.0028 - val_loss: 0.6430 - val_accuracy: 0.0026\n",
      "Epoch 7/10\n",
      "363/363 [==============================] - 0s 942us/step - loss: 0.6123 - accuracy: 0.0028 - val_loss: 0.6277 - val_accuracy: 0.0026\n",
      "Epoch 8/10\n",
      "363/363 [==============================] - 0s 944us/step - loss: 0.5982 - accuracy: 0.0028 - val_loss: 0.6139 - val_accuracy: 0.0026\n",
      "Epoch 9/10\n",
      "363/363 [==============================] - 0s 882us/step - loss: 0.5853 - accuracy: 0.0028 - val_loss: 0.5994 - val_accuracy: 0.0026\n",
      "Epoch 10/10\n",
      "363/363 [==============================] - 0s 918us/step - loss: 0.5735 - accuracy: 0.0028 - val_loss: 0.5876 - val_accuracy: 0.0026\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train,y_train, epochs=10, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(pd.DataFrame(history.history))\n",
    "#plt.grid(True)\n",
    "plt.gca().set_ylim(0, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162/162 [==============================] - 0s 636us/step - loss: 0.5969 - accuracy: 0.0037\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5969493985176086, 0.0036821705289185047]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 35ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2.313411 ],\n",
       "       [2.6926491],\n",
       "       [0.9148443]], dtype=float32)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_test[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.787, 1.58 , 0.804])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functional API "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ = keras.layers.Input(shape =X_train.shape[1:] )\n",
    "hidden_1 = keras.layers.Dense(3, activation=\"relu\")(input_)\n",
    "hidden_2 = keras.layers.Dense(3,activation=\"relu\")(hidden_1)\n",
    "concat = keras.layers.Concatenate()([input_,hidden_2])\n",
    "output = keras.layers.Dense(1)(concat)\n",
    "model = keras.Model(inputs=input_,outputs=output )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"mean_squared_error\", optimizer = \"sgd\", metrics=[\"accuracy\"] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "363/363 [==============================] - 0s 704us/step - loss: 0.9536 - accuracy: 0.0028\n",
      "Epoch 2/10\n",
      "363/363 [==============================] - 0s 704us/step - loss: 0.5611 - accuracy: 0.0028\n",
      "Epoch 3/10\n",
      "363/363 [==============================] - 0s 645us/step - loss: 0.5332 - accuracy: 0.0028\n",
      "Epoch 4/10\n",
      "363/363 [==============================] - 0s 744us/step - loss: 0.6650 - accuracy: 0.0028\n",
      "Epoch 5/10\n",
      "363/363 [==============================] - 0s 669us/step - loss: 0.5154 - accuracy: 0.0028\n",
      "Epoch 6/10\n",
      "363/363 [==============================] - 0s 685us/step - loss: 0.5107 - accuracy: 0.0028\n",
      "Epoch 7/10\n",
      "363/363 [==============================] - 0s 650us/step - loss: 0.5283 - accuracy: 0.0028\n",
      "Epoch 8/10\n",
      "363/363 [==============================] - 0s 721us/step - loss: 0.5183 - accuracy: 0.0027\n",
      "Epoch 9/10\n",
      "363/363 [==============================] - 0s 659us/step - loss: 0.5185 - accuracy: 0.0028\n",
      "Epoch 10/10\n",
      "363/363 [==============================] - 0s 684us/step - loss: 0.5090 - accuracy: 0.0028\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x289275b3400>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If you want to send a subset of the features through the wide path and a different subset (possibly overlapping) through the deep path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_A = keras.layers.Input(shape = [5])\n",
    "input_B = keras.layers.Input(shape = [6])\n",
    "hidden_1 = keras.layers.Dense(30,activation=\"relu\")(input_B)\n",
    "hidden_2= keras.layers.Dense(30, activation=\"relu\")(hidden_1)\n",
    "concat = keras.layers.Concatenate()([input_A,hidden_2])\n",
    "output = keras.layers.Dense(1)(concat)\n",
    "model = keras.Model(inputs =[input_A,input_B], outputs=output )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"mse\", optimizer = keras.optimizers.SGD(learning_rate=1e-3), metrics=[\"accuracy\"] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_A = X_train[:,:5]\n",
    "X_train_B = X_train[:,2:]\n",
    "X_valid_A =  X_valid[:,:5]\n",
    "X_valid_B = X_valid[:,2:]\n",
    "X_test_A =  X_test[:,:5]\n",
    "X_test_B = X_test[:,2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 2.4084 - accuracy: 0.0021 - val_loss: 0.9062 - val_accuracy: 0.0026\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 0s 893us/step - loss: 0.7900 - accuracy: 0.0028 - val_loss: 0.7339 - val_accuracy: 0.0026\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 0s 933us/step - loss: 0.6812 - accuracy: 0.0028 - val_loss: 0.6514 - val_accuracy: 0.0026\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 0s 951us/step - loss: 0.6199 - accuracy: 0.0028 - val_loss: 0.6074 - val_accuracy: 0.0026\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 0s 921us/step - loss: 0.5852 - accuracy: 0.0028 - val_loss: 0.5811 - val_accuracy: 0.0026\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 0s 961us/step - loss: 0.5620 - accuracy: 0.0028 - val_loss: 0.5654 - val_accuracy: 0.0026\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 0s 976us/step - loss: 0.5453 - accuracy: 0.0028 - val_loss: 0.5500 - val_accuracy: 0.0026\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 0s 950us/step - loss: 0.5322 - accuracy: 0.0028 - val_loss: 0.5386 - val_accuracy: 0.0026\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 0s 953us/step - loss: 0.5241 - accuracy: 0.0028 - val_loss: 0.5333 - val_accuracy: 0.0026\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 0s 903us/step - loss: 0.5164 - accuracy: 0.0028 - val_loss: 0.5278 - val_accuracy: 0.0026\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5113 - accuracy: 0.0028 - val_loss: 0.5357 - val_accuracy: 0.0026\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5064 - accuracy: 0.0028 - val_loss: 0.5172 - val_accuracy: 0.0026\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5022 - accuracy: 0.0028 - val_loss: 0.5126 - val_accuracy: 0.0026\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4980 - accuracy: 0.0028 - val_loss: 0.5067 - val_accuracy: 0.0026\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4962 - accuracy: 0.0028 - val_loss: 0.5055 - val_accuracy: 0.0026\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 0s 909us/step - loss: 0.4924 - accuracy: 0.0028 - val_loss: 0.5029 - val_accuracy: 0.0026\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 0s 892us/step - loss: 0.4898 - accuracy: 0.0028 - val_loss: 0.5056 - val_accuracy: 0.0026\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 0s 936us/step - loss: 0.4877 - accuracy: 0.0028 - val_loss: 0.4985 - val_accuracy: 0.0026\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 0s 922us/step - loss: 0.4848 - accuracy: 0.0028 - val_loss: 0.4949 - val_accuracy: 0.0026\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 0s 902us/step - loss: 0.4826 - accuracy: 0.0028 - val_loss: 0.4978 - val_accuracy: 0.0026\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x289275b8f10>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([X_train_A,X_train_B], y_train, epochs=20, validation_data = ([X_valid_A,X_valid_B], y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162/162 [==============================] - 0s 642us/step - loss: 0.5029 - accuracy: 0.0037\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.502906322479248, 0.0036821705289185047]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate((X_test_A,X_test_B), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 32ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2.3801677 ],\n",
       "       [3.1932583 ],\n",
       "       [0.76266915]], dtype=float32)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict((X_test_A[:3],X_test_B[:3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding extra outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_A = keras.layers.Input(shape=[5], name=\"wide_input\")\n",
    "input_B = keras.layers.Input(shape=[6], name=\"deep_input\")\n",
    "hidden1 = keras.layers.Dense(30, activation=\"relu\")(input_B)\n",
    "hidden2 = keras.layers.Dense(30, activation=\"relu\")(hidden1)\n",
    "concat = keras.layers.concatenate([input_A, hidden2])\n",
    "output = keras.layers.Dense(1, name=\"main_output\")(concat)\n",
    "aux_output = keras.layers.Dense(1, name=\"aux_output\")(hidden2)\n",
    "model = keras.models.Model(inputs=[input_A, input_B],\n",
    "                           outputs=[output, aux_output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=[\"mse\", \"mse\"], loss_weights=[0.9, 0.1], optimizer=keras.optimizers.SGD(learning_rate=1e-3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 2.9301 - main_output_loss: 2.7772 - aux_output_loss: 4.3061 - val_loss: 1.4990 - val_main_output_loss: 1.2659 - val_aux_output_loss: 3.5970\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 1.2351 - main_output_loss: 1.0428 - aux_output_loss: 2.9658 - val_loss: 1.0185 - val_main_output_loss: 0.8541 - val_aux_output_loss: 2.4981\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.9275 - main_output_loss: 0.7878 - aux_output_loss: 2.1847 - val_loss: 0.8589 - val_main_output_loss: 0.7360 - val_aux_output_loss: 1.9649\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.8087 - main_output_loss: 0.6993 - aux_output_loss: 1.7939 - val_loss: 0.7750 - val_main_output_loss: 0.6751 - val_aux_output_loss: 1.6746\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 0s 946us/step - loss: 0.7437 - main_output_loss: 0.6505 - aux_output_loss: 1.5824 - val_loss: 0.7256 - val_main_output_loss: 0.6381 - val_aux_output_loss: 1.5130\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 0s 991us/step - loss: 0.7007 - main_output_loss: 0.6167 - aux_output_loss: 1.4572 - val_loss: 0.6904 - val_main_output_loss: 0.6096 - val_aux_output_loss: 1.4179\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.6691 - main_output_loss: 0.5906 - aux_output_loss: 1.3760 - val_loss: 0.6634 - val_main_output_loss: 0.5867 - val_aux_output_loss: 1.3543\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.6447 - main_output_loss: 0.5696 - aux_output_loss: 1.3207 - val_loss: 0.6432 - val_main_output_loss: 0.5692 - val_aux_output_loss: 1.3089\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.6256 - main_output_loss: 0.5531 - aux_output_loss: 1.2779 - val_loss: 0.6256 - val_main_output_loss: 0.5533 - val_aux_output_loss: 1.2755\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 0s 971us/step - loss: 0.6097 - main_output_loss: 0.5390 - aux_output_loss: 1.2462 - val_loss: 0.6126 - val_main_output_loss: 0.5420 - val_aux_output_loss: 1.2476\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5970 - main_output_loss: 0.5278 - aux_output_loss: 1.2196 - val_loss: 0.6003 - val_main_output_loss: 0.5309 - val_aux_output_loss: 1.2252\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5858 - main_output_loss: 0.5180 - aux_output_loss: 1.1959 - val_loss: 0.5912 - val_main_output_loss: 0.5230 - val_aux_output_loss: 1.2052\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 0s 999us/step - loss: 0.5772 - main_output_loss: 0.5108 - aux_output_loss: 1.1746 - val_loss: 0.5822 - val_main_output_loss: 0.5149 - val_aux_output_loss: 1.1877\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 0s 980us/step - loss: 0.5692 - main_output_loss: 0.5040 - aux_output_loss: 1.1562 - val_loss: 0.5757 - val_main_output_loss: 0.5096 - val_aux_output_loss: 1.1705\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5624 - main_output_loss: 0.4983 - aux_output_loss: 1.1385 - val_loss: 0.5698 - val_main_output_loss: 0.5048 - val_aux_output_loss: 1.1545\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5560 - main_output_loss: 0.4932 - aux_output_loss: 1.1212 - val_loss: 0.5647 - val_main_output_loss: 0.5009 - val_aux_output_loss: 1.1388\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5505 - main_output_loss: 0.4888 - aux_output_loss: 1.1051 - val_loss: 0.5594 - val_main_output_loss: 0.4966 - val_aux_output_loss: 1.1244\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 0s 987us/step - loss: 0.5458 - main_output_loss: 0.4854 - aux_output_loss: 1.0900 - val_loss: 0.5550 - val_main_output_loss: 0.4934 - val_aux_output_loss: 1.1090\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 0s 972us/step - loss: 0.5414 - main_output_loss: 0.4822 - aux_output_loss: 1.0747 - val_loss: 0.5511 - val_main_output_loss: 0.4907 - val_aux_output_loss: 1.0947\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 0s 939us/step - loss: 0.5373 - main_output_loss: 0.4791 - aux_output_loss: 1.0609 - val_loss: 0.5471 - val_main_output_loss: 0.4878 - val_aux_output_loss: 1.0806\n"
     ]
    }
   ],
   "source": [
    "history = model.fit([X_train_A, X_train_B], [y_train, y_train], epochs=20,\n",
    "                    validation_data=([X_valid_A, X_valid_B], [y_valid, y_valid]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162/162 [==============================] - 0s 770us/step - loss: 0.5590 - main_output_loss: 0.4985 - aux_output_loss: 1.1039\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5590282082557678, 0.49848589301109314, 1.103907823562622]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate([X_test_A, X_test_B], [y_test, y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 37ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([[2.298667],\n",
       "        [3.202664],\n",
       "        [0.659891]], dtype=float32),\n",
       " array([[2.5179083],\n",
       "        [2.3659084],\n",
       "        [1.6994157]], dtype=float32)]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict([X_test_A[:3], X_test_B[:3]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"my_keras_model.h5\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"my_keras_model.h5\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Callbacks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\Admin\\ML_PATH\\my_env\\lib\\site-packages\\keras\\engine\\training.py\", line 1051, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\Admin\\ML_PATH\\my_env\\lib\\site-packages\\keras\\engine\\training.py\", line 1040, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\Admin\\ML_PATH\\my_env\\lib\\site-packages\\keras\\engine\\training.py\", line 1030, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\Admin\\ML_PATH\\my_env\\lib\\site-packages\\keras\\engine\\training.py\", line 889, in train_step\n        y_pred = self(x, training=True)\n    File \"C:\\Users\\Admin\\ML_PATH\\my_env\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\Admin\\ML_PATH\\my_env\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 200, in assert_input_compatibility\n        raise ValueError(f'Layer \"{layer_name}\" expects {len(input_spec)} input(s),'\n\n    ValueError: Layer \"model_2\" expects 2 input(s), but it received 1 input tensors. Inputs received: [<tf.Tensor 'IteratorGetNext:0' shape=(None, 8) dtype=float32>]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [67]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m checkpoint_cb \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mModelCheckpoint(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmy_keras.model.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m, save_best_only \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      2\u001b[0m early_stopping_cb \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mEarlyStopping(patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m----> 3\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_valid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_valid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mcheckpoint_cb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping_cb\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\ML_PATH\\my_env\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filesz8ap2lz.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\Users\\Admin\\ML_PATH\\my_env\\lib\\site-packages\\keras\\engine\\training.py\", line 1051, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\Admin\\ML_PATH\\my_env\\lib\\site-packages\\keras\\engine\\training.py\", line 1040, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\Admin\\ML_PATH\\my_env\\lib\\site-packages\\keras\\engine\\training.py\", line 1030, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\Admin\\ML_PATH\\my_env\\lib\\site-packages\\keras\\engine\\training.py\", line 889, in train_step\n        y_pred = self(x, training=True)\n    File \"C:\\Users\\Admin\\ML_PATH\\my_env\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\Admin\\ML_PATH\\my_env\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 200, in assert_input_compatibility\n        raise ValueError(f'Layer \"{layer_name}\" expects {len(input_spec)} input(s),'\n\n    ValueError: Layer \"model_2\" expects 2 input(s), but it received 1 input tensors. Inputs received: [<tf.Tensor 'IteratorGetNext:0' shape=(None, 8) dtype=float32>]\n"
     ]
    }
   ],
   "source": [
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_keras.model.h5\", save_best_only = True)\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
    "history = model.fit(X_train, y_train, epochs=10, validation_data=(X_valid, y_valid), callbacks=[checkpoint_cb, early_stopping_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = fetch_california_housing()\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data, housing.target)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train =scaler.fit_transform(X_train)\n",
    "X_valid = scaler.transform(X_valid)\n",
    "X_test =scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(n_hidden=1, n_neurons=30, learning_rate=3e-3, input_shape=[8]):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.InputLayer(input_shape=input_shape))\n",
    "    for layer in range(n_hidden):\n",
    "        model.add(keras.layers.Dense(n_neurons, activation=\"relu\"))\n",
    "        model.add(keras.layers.Dense(1))\n",
    "        optimizer = keras.optimizers.SGD(lr=learning_rate)\n",
    "        model.compile(loss=\"mse\", optimizer=optimizer)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_1116\\1709004121.py:1: DeprecationWarning: KerasRegressor is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  keras_reg = keras.wrappers.scikit_learn.KerasRegressor(build_model)\n"
     ]
    }
   ],
   "source": [
    "keras_reg = keras.wrappers.scikit_learn.KerasRegressor(build_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "  1/363 [..............................] - ETA: 49s - loss: 6.7856"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\ML_PATH\\my_env\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "363/363 [==============================] - 0s 948us/step - loss: 1.2397 - val_loss: 0.6986\n",
      "Epoch 2/100\n",
      "363/363 [==============================] - 0s 804us/step - loss: 0.6286 - val_loss: 0.5900\n",
      "Epoch 3/100\n",
      "363/363 [==============================] - 0s 790us/step - loss: 0.5651 - val_loss: 0.5444\n",
      "Epoch 4/100\n",
      "363/363 [==============================] - 0s 839us/step - loss: 0.5274 - val_loss: 0.5158\n",
      "Epoch 5/100\n",
      "363/363 [==============================] - 0s 860us/step - loss: 0.5029 - val_loss: 0.4984\n",
      "Epoch 6/100\n",
      "363/363 [==============================] - 0s 842us/step - loss: 0.4849 - val_loss: 0.4802\n",
      "Epoch 7/100\n",
      "363/363 [==============================] - 0s 815us/step - loss: 0.4701 - val_loss: 0.4664\n",
      "Epoch 8/100\n",
      "363/363 [==============================] - 0s 849us/step - loss: 0.4619 - val_loss: 0.4582\n",
      "Epoch 9/100\n",
      "363/363 [==============================] - 0s 831us/step - loss: 0.4526 - val_loss: 0.4500\n",
      "Epoch 10/100\n",
      "363/363 [==============================] - 0s 817us/step - loss: 0.4464 - val_loss: 0.4438\n",
      "Epoch 11/100\n",
      "363/363 [==============================] - 0s 851us/step - loss: 0.4395 - val_loss: 0.4401\n",
      "Epoch 12/100\n",
      "363/363 [==============================] - 0s 792us/step - loss: 0.4349 - val_loss: 0.4356\n",
      "Epoch 13/100\n",
      "363/363 [==============================] - 0s 837us/step - loss: 0.4303 - val_loss: 0.4288\n",
      "Epoch 14/100\n",
      "363/363 [==============================] - 0s 810us/step - loss: 0.4261 - val_loss: 0.4246\n",
      "Epoch 15/100\n",
      "363/363 [==============================] - 0s 848us/step - loss: 0.4232 - val_loss: 0.4228\n",
      "Epoch 16/100\n",
      "363/363 [==============================] - 0s 781us/step - loss: 0.4205 - val_loss: 0.4205\n",
      "Epoch 17/100\n",
      "363/363 [==============================] - 0s 849us/step - loss: 0.4162 - val_loss: 0.4178\n",
      "Epoch 18/100\n",
      "363/363 [==============================] - 0s 899us/step - loss: 0.4165 - val_loss: 0.4123\n",
      "Epoch 19/100\n",
      "363/363 [==============================] - 0s 794us/step - loss: 0.4121 - val_loss: 0.4148\n",
      "Epoch 20/100\n",
      "363/363 [==============================] - 0s 880us/step - loss: 0.4079 - val_loss: 0.4105\n",
      "Epoch 21/100\n",
      "363/363 [==============================] - 0s 781us/step - loss: 0.4052 - val_loss: 0.4081\n",
      "Epoch 22/100\n",
      "363/363 [==============================] - 0s 888us/step - loss: 0.4047 - val_loss: 0.4046\n",
      "Epoch 23/100\n",
      "363/363 [==============================] - 0s 899us/step - loss: 0.4013 - val_loss: 0.4072\n",
      "Epoch 24/100\n",
      "363/363 [==============================] - 0s 820us/step - loss: 0.4050 - val_loss: 0.4014\n",
      "Epoch 25/100\n",
      "363/363 [==============================] - 0s 822us/step - loss: 0.4030 - val_loss: 0.3989\n",
      "Epoch 26/100\n",
      "363/363 [==============================] - 0s 817us/step - loss: 0.4003 - val_loss: 0.3959\n",
      "Epoch 27/100\n",
      "363/363 [==============================] - 0s 792us/step - loss: 0.3929 - val_loss: 0.3942\n",
      "Epoch 28/100\n",
      "363/363 [==============================] - 0s 791us/step - loss: 0.3912 - val_loss: 0.3926\n",
      "Epoch 29/100\n",
      "363/363 [==============================] - 0s 834us/step - loss: 0.3904 - val_loss: 0.3934\n",
      "Epoch 30/100\n",
      "363/363 [==============================] - 0s 809us/step - loss: 0.3892 - val_loss: 0.3907\n",
      "Epoch 31/100\n",
      "363/363 [==============================] - 0s 845us/step - loss: 0.3939 - val_loss: 0.3902\n",
      "Epoch 32/100\n",
      "363/363 [==============================] - 0s 847us/step - loss: 0.3848 - val_loss: 0.3868\n",
      "Epoch 33/100\n",
      "363/363 [==============================] - 0s 813us/step - loss: 0.3848 - val_loss: 0.3867\n",
      "Epoch 34/100\n",
      "363/363 [==============================] - 0s 833us/step - loss: 0.3824 - val_loss: 0.3868\n",
      "Epoch 35/100\n",
      "363/363 [==============================] - 0s 849us/step - loss: 0.3820 - val_loss: 0.3851\n",
      "Epoch 36/100\n",
      "363/363 [==============================] - 0s 933us/step - loss: 0.3846 - val_loss: 0.3824\n",
      "Epoch 37/100\n",
      "363/363 [==============================] - 0s 839us/step - loss: 0.3784 - val_loss: 0.3814\n",
      "Epoch 38/100\n",
      "363/363 [==============================] - 0s 823us/step - loss: 0.3793 - val_loss: 0.3805\n",
      "Epoch 39/100\n",
      "363/363 [==============================] - 0s 842us/step - loss: 0.3759 - val_loss: 0.3795\n",
      "Epoch 40/100\n",
      "363/363 [==============================] - 0s 930us/step - loss: 0.3751 - val_loss: 0.3781\n",
      "Epoch 41/100\n",
      "363/363 [==============================] - 0s 934us/step - loss: 0.3765 - val_loss: 0.3815\n",
      "Epoch 42/100\n",
      "363/363 [==============================] - 0s 846us/step - loss: 0.3723 - val_loss: 0.3761\n",
      "Epoch 43/100\n",
      "363/363 [==============================] - 0s 845us/step - loss: 0.3738 - val_loss: 0.3801\n",
      "Epoch 44/100\n",
      "363/363 [==============================] - 0s 894us/step - loss: 0.3714 - val_loss: 0.3745\n",
      "Epoch 45/100\n",
      "363/363 [==============================] - 0s 800us/step - loss: 0.3710 - val_loss: 0.3731\n",
      "Epoch 46/100\n",
      "363/363 [==============================] - 0s 818us/step - loss: 0.3687 - val_loss: 0.3755\n",
      "Epoch 47/100\n",
      "363/363 [==============================] - 0s 800us/step - loss: 0.3680 - val_loss: 0.3708\n",
      "Epoch 48/100\n",
      "363/363 [==============================] - 0s 849us/step - loss: 0.3667 - val_loss: 0.3712\n",
      "Epoch 49/100\n",
      "363/363 [==============================] - 0s 818us/step - loss: 0.3656 - val_loss: 0.3710\n",
      "Epoch 50/100\n",
      "363/363 [==============================] - 0s 819us/step - loss: 0.3645 - val_loss: 0.3687\n",
      "Epoch 51/100\n",
      "363/363 [==============================] - 0s 819us/step - loss: 0.3641 - val_loss: 0.3685\n",
      "Epoch 52/100\n",
      "363/363 [==============================] - 0s 807us/step - loss: 0.3669 - val_loss: 0.3691\n",
      "Epoch 53/100\n",
      "363/363 [==============================] - 0s 822us/step - loss: 0.3658 - val_loss: 0.3686\n",
      "Epoch 54/100\n",
      "363/363 [==============================] - 0s 938us/step - loss: 0.3611 - val_loss: 0.3670\n",
      "Epoch 55/100\n",
      "363/363 [==============================] - 0s 844us/step - loss: 0.3618 - val_loss: 0.3666\n",
      "Epoch 56/100\n",
      "363/363 [==============================] - 0s 823us/step - loss: 0.3598 - val_loss: 0.3656\n",
      "Epoch 57/100\n",
      "363/363 [==============================] - 0s 863us/step - loss: 0.3596 - val_loss: 0.3672\n",
      "Epoch 58/100\n",
      "363/363 [==============================] - 0s 885us/step - loss: 0.3579 - val_loss: 0.3642\n",
      "Epoch 59/100\n",
      "363/363 [==============================] - 0s 820us/step - loss: 0.3584 - val_loss: 0.3783\n",
      "Epoch 60/100\n",
      "363/363 [==============================] - 0s 850us/step - loss: 0.3583 - val_loss: 0.3644\n",
      "Epoch 61/100\n",
      "363/363 [==============================] - 0s 813us/step - loss: 0.3603 - val_loss: 0.3613\n",
      "Epoch 62/100\n",
      "363/363 [==============================] - 0s 911us/step - loss: 0.3570 - val_loss: 0.3640\n",
      "Epoch 63/100\n",
      "363/363 [==============================] - 0s 814us/step - loss: 0.3569 - val_loss: 0.3693\n",
      "Epoch 64/100\n",
      "363/363 [==============================] - 0s 822us/step - loss: 0.3610 - val_loss: 0.3672\n",
      "Epoch 65/100\n",
      "363/363 [==============================] - 0s 858us/step - loss: 0.3632 - val_loss: 0.3624\n",
      "Epoch 66/100\n",
      "363/363 [==============================] - 0s 869us/step - loss: 0.3526 - val_loss: 0.3609\n",
      "Epoch 67/100\n",
      "363/363 [==============================] - 0s 825us/step - loss: 0.3522 - val_loss: 0.3607\n",
      "Epoch 68/100\n",
      "363/363 [==============================] - 0s 801us/step - loss: 0.3550 - val_loss: 0.3604\n",
      "Epoch 69/100\n",
      "363/363 [==============================] - 0s 906us/step - loss: 0.3504 - val_loss: 0.3588\n",
      "Epoch 70/100\n",
      "363/363 [==============================] - 0s 925us/step - loss: 0.3520 - val_loss: 0.3596\n",
      "Epoch 71/100\n",
      "363/363 [==============================] - 0s 924us/step - loss: 0.3507 - val_loss: 0.3614\n",
      "Epoch 72/100\n",
      "363/363 [==============================] - 0s 817us/step - loss: 0.3513 - val_loss: 0.3572\n",
      "Epoch 73/100\n",
      "363/363 [==============================] - 0s 825us/step - loss: 0.3491 - val_loss: 0.3590\n",
      "Epoch 74/100\n",
      "363/363 [==============================] - 0s 836us/step - loss: 0.3480 - val_loss: 0.3565\n",
      "Epoch 75/100\n",
      "363/363 [==============================] - 0s 849us/step - loss: 0.3473 - val_loss: 0.3565\n",
      "Epoch 76/100\n",
      "363/363 [==============================] - 0s 798us/step - loss: 0.3465 - val_loss: 0.3590\n",
      "Epoch 77/100\n",
      "363/363 [==============================] - 0s 852us/step - loss: 0.3464 - val_loss: 0.3566\n",
      "Epoch 78/100\n",
      "363/363 [==============================] - 0s 837us/step - loss: 0.3448 - val_loss: 0.3540\n",
      "Epoch 79/100\n",
      "363/363 [==============================] - 0s 835us/step - loss: 0.3465 - val_loss: 0.3557\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/100\n",
      "363/363 [==============================] - 0s 844us/step - loss: 0.3457 - val_loss: 0.3562\n",
      "Epoch 81/100\n",
      "363/363 [==============================] - 0s 779us/step - loss: 0.3500 - val_loss: 0.3560\n",
      "Epoch 82/100\n",
      "363/363 [==============================] - 0s 944us/step - loss: 0.3569 - val_loss: 0.3550\n",
      "Epoch 83/100\n",
      "363/363 [==============================] - 0s 846us/step - loss: 0.3517 - val_loss: 0.3566\n",
      "Epoch 84/100\n",
      "363/363 [==============================] - 0s 794us/step - loss: 0.3438 - val_loss: 0.3537\n",
      "Epoch 85/100\n",
      "363/363 [==============================] - 0s 899us/step - loss: 0.3417 - val_loss: 0.3520\n",
      "Epoch 86/100\n",
      "363/363 [==============================] - 0s 854us/step - loss: 0.3417 - val_loss: 0.3538\n",
      "Epoch 87/100\n",
      "363/363 [==============================] - 0s 814us/step - loss: 0.3432 - val_loss: 0.3525\n",
      "Epoch 88/100\n",
      "363/363 [==============================] - 0s 899us/step - loss: 0.3412 - val_loss: 0.3515\n",
      "Epoch 89/100\n",
      "363/363 [==============================] - 0s 857us/step - loss: 0.3401 - val_loss: 0.3531\n",
      "Epoch 90/100\n",
      "363/363 [==============================] - 0s 801us/step - loss: 0.3437 - val_loss: 0.3535\n",
      "Epoch 91/100\n",
      "363/363 [==============================] - 0s 887us/step - loss: 0.3533 - val_loss: 0.3506\n",
      "Epoch 92/100\n",
      "363/363 [==============================] - 0s 840us/step - loss: 0.3430 - val_loss: 0.3483\n",
      "Epoch 93/100\n",
      "363/363 [==============================] - 0s 821us/step - loss: 0.3379 - val_loss: 0.3492\n",
      "Epoch 94/100\n",
      "363/363 [==============================] - 0s 831us/step - loss: 0.3368 - val_loss: 0.3485\n",
      "Epoch 95/100\n",
      "363/363 [==============================] - 0s 950us/step - loss: 0.3401 - val_loss: 0.3509\n",
      "Epoch 96/100\n",
      "363/363 [==============================] - 0s 863us/step - loss: 0.3396 - val_loss: 0.3521\n",
      "Epoch 97/100\n",
      "363/363 [==============================] - 0s 847us/step - loss: 0.3352 - val_loss: 0.3496\n",
      "Epoch 98/100\n",
      "363/363 [==============================] - 0s 855us/step - loss: 0.3347 - val_loss: 0.3461\n",
      "Epoch 99/100\n",
      "363/363 [==============================] - 0s 855us/step - loss: 0.3345 - val_loss: 0.3475\n",
      "Epoch 100/100\n",
      "363/363 [==============================] - 0s 812us/step - loss: 0.3341 - val_loss: 0.3481\n",
      "162/162 [==============================] - 0s 667us/step - loss: 0.3532\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000028928B34700> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 30ms/step\n"
     ]
    }
   ],
   "source": [
    "keras_reg.fit(X_train, y_train, epochs=100, validation_data=(X_valid, y_valid), callbacks=[keras.callbacks.EarlyStopping(patience=10)])\n",
    "mse_test = keras_reg.score(X_test, y_test)\n",
    "y_pred = keras_reg.predict(X_test[:3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\ML_PATH\\my_env\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 1s 1ms/step - loss: 1.8301 - val_loss: 0.8513\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.6770 - val_loss: 0.6174\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 952us/step - loss: 0.5923 - val_loss: 0.5790\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5597 - val_loss: 0.5486\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5319 - val_loss: 0.5241\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 965us/step - loss: 0.5080 - val_loss: 0.5050\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4890 - val_loss: 0.4872\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4725 - val_loss: 0.4716\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 967us/step - loss: 0.4580 - val_loss: 0.4577\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 966us/step - loss: 0.4453 - val_loss: 0.4464\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 913us/step - loss: 0.4344 - val_loss: 0.4379\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 999us/step - loss: 0.4247 - val_loss: 0.4270\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 955us/step - loss: 0.4158 - val_loss: 0.4207\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 975us/step - loss: 0.4084 - val_loss: 0.4123\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 982us/step - loss: 0.4015 - val_loss: 0.4089\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 979us/step - loss: 0.3958 - val_loss: 0.4030\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.3910 - val_loss: 0.4033\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 953us/step - loss: 0.3890 - val_loss: 0.3974\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 955us/step - loss: 0.3833 - val_loss: 0.3927\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3800 - val_loss: 0.3902\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 969us/step - loss: 0.3768 - val_loss: 0.3885\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 930us/step - loss: 0.3735 - val_loss: 0.3846\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 975us/step - loss: 0.3700 - val_loss: 0.3855\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 953us/step - loss: 0.3669 - val_loss: 0.3843\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3633 - val_loss: 0.3809\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 937us/step - loss: 0.3610 - val_loss: 0.3814\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 898us/step - loss: 0.3576 - val_loss: 0.3742\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 927us/step - loss: 0.3538 - val_loss: 0.3817\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 929us/step - loss: 0.3761 - val_loss: 0.3843\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 987us/step - loss: 0.3604 - val_loss: 0.3741\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 987us/step - loss: 0.3534 - val_loss: 0.3694\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 954us/step - loss: 0.3505 - val_loss: 0.3673\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 985us/step - loss: 0.3469 - val_loss: 0.3648\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 911us/step - loss: 0.3445 - val_loss: 0.3630\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 982us/step - loss: 0.3424 - val_loss: 0.3630\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 974us/step - loss: 0.3406 - val_loss: 0.3587\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 968us/step - loss: 0.3390 - val_loss: 0.3631\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3373 - val_loss: 0.3580\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 967us/step - loss: 0.3346 - val_loss: 0.3651\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 985us/step - loss: 0.3338 - val_loss: 0.3576\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 965us/step - loss: 0.3326 - val_loss: 0.3592\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 918us/step - loss: 0.3318 - val_loss: 0.3530\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 982us/step - loss: 0.3273 - val_loss: 0.3517\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3279 - val_loss: 0.3561\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3271 - val_loss: 0.3517\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3272 - val_loss: 0.3498\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3251 - val_loss: 0.3477\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3247 - val_loss: 0.3471\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3235 - val_loss: 0.3470\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3204 - val_loss: 0.3471\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 976us/step - loss: 0.3188 - val_loss: 0.3451\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 895us/step - loss: 0.3174 - val_loss: 0.3483\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 971us/step - loss: 0.3166 - val_loss: 0.3442\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 956us/step - loss: 0.3177 - val_loss: 0.3471\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3181 - val_loss: 0.3436\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 969us/step - loss: 0.3142 - val_loss: 0.3459\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3146 - val_loss: 0.3418\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3134 - val_loss: 0.3399\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 988us/step - loss: 0.3144 - val_loss: 0.3430\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.3114 - val_loss: 0.3389\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 967us/step - loss: 0.3138 - val_loss: 0.3512\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 888us/step - loss: 0.3129 - val_loss: 0.3481\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 965us/step - loss: 0.3113 - val_loss: 0.3408\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 894us/step - loss: 0.3112 - val_loss: 0.3396\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 952us/step - loss: 0.3113 - val_loss: 0.3409\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 985us/step - loss: 0.3125 - val_loss: 0.3367\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3095 - val_loss: 0.3386\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 969us/step - loss: 0.3092 - val_loss: 0.3365\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 966us/step - loss: 0.3073 - val_loss: 0.3372\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3082 - val_loss: 0.3345\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3057 - val_loss: 0.3355\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 999us/step - loss: 0.3061 - val_loss: 0.3348\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 972us/step - loss: 0.3071 - val_loss: 0.3337\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 937us/step - loss: 0.3037 - val_loss: 0.3333\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 971us/step - loss: 0.3043 - val_loss: 0.3323\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3047 - val_loss: 0.3356\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 960us/step - loss: 0.3049 - val_loss: 0.3341\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 961us/step - loss: 0.3046 - val_loss: 0.3318\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 982us/step - loss: 0.3051 - val_loss: 0.3359\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 965us/step - loss: 0.3031 - val_loss: 0.3319\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 979us/step - loss: 0.3022 - val_loss: 0.3314\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 940us/step - loss: 0.3034 - val_loss: 0.3394\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 944us/step - loss: 0.3029 - val_loss: 0.3351\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 919us/step - loss: 0.3030 - val_loss: 0.3342\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3021 - val_loss: 0.3325\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 966us/step - loss: 0.3004 - val_loss: 0.3298\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3019 - val_loss: 0.3320\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3008 - val_loss: 0.3293\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3013 - val_loss: 0.3293\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2992 - val_loss: 0.3719\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 994us/step - loss: 0.3009 - val_loss: 0.3574\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3025 - val_loss: 0.3316\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2977 - val_loss: 0.3289\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2985 - val_loss: 0.3320\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 987us/step - loss: 0.2978 - val_loss: 0.3287\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 959us/step - loss: 0.2992 - val_loss: 0.3295\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 975us/step - loss: 0.3126 - val_loss: 0.3347\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 965us/step - loss: 0.3024 - val_loss: 0.3282\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2992 - val_loss: 0.3303\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 988us/step - loss: 0.2990 - val_loss: 0.3312\n",
      "121/121 [==============================] - 0s 703us/step - loss: 0.3409\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\ML_PATH\\my_env\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 1ms/step - loss: 1.6982 - val_loss: 0.8500\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 948us/step - loss: 0.7314 - val_loss: 0.6471\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 940us/step - loss: 0.6160 - val_loss: 0.5875\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 937us/step - loss: 0.5616 - val_loss: 0.5441\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 956us/step - loss: 0.5234 - val_loss: 0.5126\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 955us/step - loss: 0.4955 - val_loss: 0.4920\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 887us/step - loss: 0.4759 - val_loss: 0.4688\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 891us/step - loss: 0.4583 - val_loss: 0.4549\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 951us/step - loss: 0.4459 - val_loss: 0.4425\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 936us/step - loss: 0.4347 - val_loss: 0.4359\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 792us/step - loss: 0.4251 - val_loss: 0.4239\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.4173 - val_loss: 0.4168\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 952us/step - loss: 0.4102 - val_loss: 0.4126\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 910us/step - loss: 0.4033 - val_loss: 0.4049\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 943us/step - loss: 0.3982 - val_loss: 0.4038\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 923us/step - loss: 0.3929 - val_loss: 0.3985\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 0.3880 - val_loss: 0.3917\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3836 - val_loss: 0.3944\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 973us/step - loss: 0.3803 - val_loss: 0.3903\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 941us/step - loss: 0.3771 - val_loss: 0.3851\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 946us/step - loss: 0.3723 - val_loss: 0.3919\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 850us/step - loss: 0.3706 - val_loss: 0.3785\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 848us/step - loss: 0.3677 - val_loss: 0.3770\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 935us/step - loss: 0.3642 - val_loss: 0.3844\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 929us/step - loss: 0.3627 - val_loss: 0.3754\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 977us/step - loss: 0.3604 - val_loss: 0.3743\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 975us/step - loss: 0.3580 - val_loss: 0.3721\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3561 - val_loss: 0.3744\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 930us/step - loss: 0.3545 - val_loss: 0.3710\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.3529 - val_loss: 0.3897\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3516 - val_loss: 0.3726\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 993us/step - loss: 0.3497 - val_loss: 0.3684\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 897us/step - loss: 0.3492 - val_loss: 0.3645\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.3479 - val_loss: 0.3625\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 893us/step - loss: 0.3456 - val_loss: 0.3622\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 910us/step - loss: 0.3465 - val_loss: 0.3616\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 947us/step - loss: 0.3463 - val_loss: 0.3665\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 988us/step - loss: 0.3486 - val_loss: 0.3628\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 963us/step - loss: 0.3539 - val_loss: 0.3616\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 941us/step - loss: 0.3599 - val_loss: 0.3652\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 924us/step - loss: 0.3721 - val_loss: 0.3576\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 986us/step - loss: 0.3578 - val_loss: 0.3585\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3421 - val_loss: 0.3674\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3396 - val_loss: 0.3593\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 951us/step - loss: 0.3364 - val_loss: 0.3606\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3352 - val_loss: 0.3598\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3348 - val_loss: 0.3534\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 940us/step - loss: 0.3345 - val_loss: 0.3560\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 987us/step - loss: 0.3336 - val_loss: 0.3530\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3319 - val_loss: 0.3635\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 958us/step - loss: 0.3317 - val_loss: 0.3582\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 999us/step - loss: 0.3300 - val_loss: 0.3569\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3302 - val_loss: 0.3529\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3294 - val_loss: 0.3609\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3296 - val_loss: 0.3517\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 959us/step - loss: 0.3281 - val_loss: 0.3609\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 1000us/step - loss: 0.3269 - val_loss: 0.3589\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3266 - val_loss: 0.3526\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3258 - val_loss: 0.3549\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3246 - val_loss: 0.3663\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 891us/step - loss: 0.3246 - val_loss: 0.3552\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 988us/step - loss: 0.3250 - val_loss: 0.3517\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 940us/step - loss: 0.3228 - val_loss: 0.3520\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 992us/step - loss: 0.3219 - val_loss: 0.3591\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 928us/step - loss: 0.3226 - val_loss: 0.3514\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 993us/step - loss: 0.3215 - val_loss: 0.3480\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3204 - val_loss: 0.3477\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 999us/step - loss: 0.3215 - val_loss: 0.3573\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 985us/step - loss: 0.3190 - val_loss: 0.3591\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3207 - val_loss: 0.3543\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3190 - val_loss: 0.3537\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3185 - val_loss: 0.3524\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 935us/step - loss: 0.3186 - val_loss: 0.3488\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3181 - val_loss: 0.3471\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3166 - val_loss: 0.3437\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 983us/step - loss: 0.3169 - val_loss: 0.3548\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 952us/step - loss: 0.3161 - val_loss: 0.3461\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3166 - val_loss: 0.3552\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3154 - val_loss: 0.3454\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 943us/step - loss: 0.3158 - val_loss: 0.3580\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 940us/step - loss: 0.3149 - val_loss: 0.3476\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 991us/step - loss: 0.3163 - val_loss: 0.3444\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 936us/step - loss: 0.3135 - val_loss: 0.3466\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 904us/step - loss: 0.3139 - val_loss: 0.3498\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3130 - val_loss: 0.3459\n",
      "121/121 [==============================] - 0s 644us/step - loss: 0.3320\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\ML_PATH\\my_env\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 1ms/step - loss: 2.0527 - val_loss: 1.2943\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 972us/step - loss: 1.1372 - val_loss: 0.8810\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7164 - val_loss: 0.6264\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6214 - val_loss: 0.5866\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5856 - val_loss: 0.5570\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5565 - val_loss: 0.5321\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5326 - val_loss: 0.5111\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5113 - val_loss: 0.4959\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4933 - val_loss: 0.4791\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4786 - val_loss: 0.4666\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4666 - val_loss: 0.4580\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4569 - val_loss: 0.4482\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4494 - val_loss: 0.4420\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4416 - val_loss: 0.4353\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 0.4357 - val_loss: 0.4323\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 906us/step - loss: 0.4305 - val_loss: 0.4256\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 973us/step - loss: 0.4251 - val_loss: 0.4205\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 967us/step - loss: 0.4208 - val_loss: 0.4189\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 963us/step - loss: 0.4168 - val_loss: 0.4138\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 908us/step - loss: 0.4126 - val_loss: 0.4092\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 955us/step - loss: 0.4096 - val_loss: 0.4070\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 951us/step - loss: 0.4047 - val_loss: 0.4052\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 995us/step - loss: 0.4014 - val_loss: 0.4016\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 891us/step - loss: 0.3969 - val_loss: 0.3978\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 979us/step - loss: 0.3935 - val_loss: 0.3936\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 970us/step - loss: 0.3891 - val_loss: 0.3917\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 999us/step - loss: 0.3868 - val_loss: 0.3879\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 0.3830 - val_loss: 0.3873\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 914us/step - loss: 0.3796 - val_loss: 0.3842\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3760 - val_loss: 0.3801\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 970us/step - loss: 0.3733 - val_loss: 0.3765\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 937us/step - loss: 0.3706 - val_loss: 0.3758\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 921us/step - loss: 0.3680 - val_loss: 0.3793\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 997us/step - loss: 0.3653 - val_loss: 0.3769\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3631 - val_loss: 0.3692\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3614 - val_loss: 0.3674\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3597 - val_loss: 0.3673\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3577 - val_loss: 0.3623\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 998us/step - loss: 0.3557 - val_loss: 0.3598\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3540 - val_loss: 0.3561\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3518 - val_loss: 0.3618\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3494 - val_loss: 0.3576\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3489 - val_loss: 0.3547\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3475 - val_loss: 0.3532\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 963us/step - loss: 0.3470 - val_loss: 0.3516\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3449 - val_loss: 0.3481\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3417 - val_loss: 0.3472\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 997us/step - loss: 0.3423 - val_loss: 0.3465\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3401 - val_loss: 0.3459\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3373 - val_loss: 0.3478\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 935us/step - loss: 0.3377 - val_loss: 0.3483\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 977us/step - loss: 0.3376 - val_loss: 0.3455\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3353 - val_loss: 0.3424\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 907us/step - loss: 0.3314 - val_loss: 0.3417\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3356 - val_loss: 0.3418\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 949us/step - loss: 0.3351 - val_loss: 0.3461\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3300 - val_loss: 0.3431\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3329 - val_loss: 0.3410\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3314 - val_loss: 0.3403\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3290 - val_loss: 0.3376\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3254 - val_loss: 0.3381\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3257 - val_loss: 0.3421\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3254 - val_loss: 0.3373\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 973us/step - loss: 0.3256 - val_loss: 0.3531\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3246 - val_loss: 0.3370\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 999us/step - loss: 0.3221 - val_loss: 0.3355\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3241 - val_loss: 0.3410\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 962us/step - loss: 0.3214 - val_loss: 0.3351\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3234 - val_loss: 0.3347\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3197 - val_loss: 0.3368\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3228 - val_loss: 0.3443\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3208 - val_loss: 0.3362\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3217 - val_loss: 0.3405\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3196 - val_loss: 0.3390\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3161 - val_loss: 0.3365\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 939us/step - loss: 0.3160 - val_loss: 0.3484\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3186 - val_loss: 0.3359\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3207 - val_loss: 0.3379\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3167 - val_loss: 0.3334\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3174 - val_loss: 0.3370\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 991us/step - loss: 0.3170 - val_loss: 0.3420\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 991us/step - loss: 0.3131 - val_loss: 0.3332\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3126 - val_loss: 0.3351\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 979us/step - loss: 0.3181 - val_loss: 0.3346\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 951us/step - loss: 0.3138 - val_loss: 0.3398\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3130 - val_loss: 0.3359\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 989us/step - loss: 0.3103 - val_loss: 0.3290\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3096 - val_loss: 0.3308\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 984us/step - loss: 0.3148 - val_loss: 0.3367\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 999us/step - loss: 0.3135 - val_loss: 0.3379\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3114 - val_loss: 0.3313\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3107 - val_loss: 0.3381\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3103 - val_loss: 0.3288\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3211 - val_loss: 0.3534\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3120 - val_loss: 0.3303\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 977us/step - loss: 0.3120 - val_loss: 0.3391\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 988us/step - loss: 0.3095 - val_loss: 0.3291\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3125 - val_loss: 0.3431\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3078 - val_loss: 0.3321\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3076 - val_loss: 0.3333\n",
      "121/121 [==============================] - 0s 653us/step - loss: 0.3152\n",
      "Epoch 1/100\n",
      "  1/242 [..............................] - ETA: 37s - loss: 7.0829"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\ML_PATH\\my_env\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 1ms/step - loss: 1.4378 - val_loss: 0.6833\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 882us/step - loss: 0.6487 - val_loss: 0.6147\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.5871 - val_loss: 0.5750\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 982us/step - loss: 0.5569 - val_loss: 0.5545\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5322 - val_loss: 0.5314\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5138 - val_loss: 0.5107\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5059 - val_loss: 0.5058\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4891 - val_loss: 0.4889\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4819 - val_loss: 0.4823\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4755 - val_loss: 0.4780\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 916us/step - loss: 0.4655 - val_loss: 0.4674\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 921us/step - loss: 0.4572 - val_loss: 0.4587\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 956us/step - loss: 0.4589 - val_loss: 0.4554\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 943us/step - loss: 0.4478 - val_loss: 0.4508\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 912us/step - loss: 0.4438 - val_loss: 0.4542\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 985us/step - loss: 0.4471 - val_loss: 0.4446\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 919us/step - loss: 0.4424 - val_loss: 0.4419\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 921us/step - loss: 0.4391 - val_loss: 0.4390\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 962us/step - loss: 0.4344 - val_loss: 0.4340\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 862us/step - loss: 0.4293 - val_loss: 0.4321\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 907us/step - loss: 0.4258 - val_loss: 0.4276\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 939us/step - loss: 0.4224 - val_loss: 0.4252\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 927us/step - loss: 0.4228 - val_loss: 0.4290\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 929us/step - loss: 0.4248 - val_loss: 0.4233\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 944us/step - loss: 0.4173 - val_loss: 0.4199\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.4152 - val_loss: 0.4208\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 943us/step - loss: 0.4187 - val_loss: 0.4177\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 983us/step - loss: 0.4151 - val_loss: 0.4131\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 939us/step - loss: 0.4170 - val_loss: 0.4128\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 898us/step - loss: 0.4072 - val_loss: 0.4109\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4058 - val_loss: 0.4174\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 849us/step - loss: 0.4247 - val_loss: 0.4078\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 934us/step - loss: 0.4133 - val_loss: 0.4099\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 959us/step - loss: 0.4011 - val_loss: 0.4045\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 906us/step - loss: 0.3986 - val_loss: 0.4092\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 903us/step - loss: 0.3974 - val_loss: 0.4016\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 899us/step - loss: 0.3953 - val_loss: 0.4004\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 921us/step - loss: 0.3928 - val_loss: 0.3981\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 0.3904 - val_loss: 0.3987\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 920us/step - loss: 0.3899 - val_loss: 0.3954\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 937us/step - loss: 0.3889 - val_loss: 0.3981\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 888us/step - loss: 0.3882 - val_loss: 0.3921\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 916us/step - loss: 0.3853 - val_loss: 0.3928\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 925us/step - loss: 0.3843 - val_loss: 0.3952\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 947us/step - loss: 0.3834 - val_loss: 0.3915\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 920us/step - loss: 0.3819 - val_loss: 0.3888\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 926us/step - loss: 0.3795 - val_loss: 0.3897\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 898us/step - loss: 0.3790 - val_loss: 0.3874\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 903us/step - loss: 0.3775 - val_loss: 0.3860\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 930us/step - loss: 0.3763 - val_loss: 0.3846\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 914us/step - loss: 0.3760 - val_loss: 0.3840\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 959us/step - loss: 0.3762 - val_loss: 0.3844\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 911us/step - loss: 0.3737 - val_loss: 0.3828\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 906us/step - loss: 0.3752 - val_loss: 0.3821\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3769 - val_loss: 0.3806\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3718 - val_loss: 0.3798\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 971us/step - loss: 0.3697 - val_loss: 0.3782\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 950us/step - loss: 0.3680 - val_loss: 0.3789\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 937us/step - loss: 0.3690 - val_loss: 0.3769\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 903us/step - loss: 0.3665 - val_loss: 0.3777\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 903us/step - loss: 0.3660 - val_loss: 0.3749\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 947us/step - loss: 0.3652 - val_loss: 0.3757\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 888us/step - loss: 0.3657 - val_loss: 0.3766\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 908us/step - loss: 0.3659 - val_loss: 0.3745\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 949us/step - loss: 0.3634 - val_loss: 0.3770\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 938us/step - loss: 0.3629 - val_loss: 0.3733\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 971us/step - loss: 0.3610 - val_loss: 0.3729\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 957us/step - loss: 0.3612 - val_loss: 0.3755\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 930us/step - loss: 0.3608 - val_loss: 0.3717\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 965us/step - loss: 0.3642 - val_loss: 0.3710\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 961us/step - loss: 0.3610 - val_loss: 0.3715\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 912us/step - loss: 0.3624 - val_loss: 0.3698\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 975us/step - loss: 0.3649 - val_loss: 0.3696\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 917us/step - loss: 0.3641 - val_loss: 0.3684\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 873us/step - loss: 0.3571 - val_loss: 0.3687\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 966us/step - loss: 0.3558 - val_loss: 0.3674\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3557 - val_loss: 0.3667\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3549 - val_loss: 0.3668\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3542 - val_loss: 0.3690\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 964us/step - loss: 0.3528 - val_loss: 0.3662\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 951us/step - loss: 0.3523 - val_loss: 0.3692\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 953us/step - loss: 0.3515 - val_loss: 0.3670\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 935us/step - loss: 0.3510 - val_loss: 0.3649\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 916us/step - loss: 0.3501 - val_loss: 0.3655\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 888us/step - loss: 0.3503 - val_loss: 0.3642\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 900us/step - loss: 0.3502 - val_loss: 0.3636\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 914us/step - loss: 0.3485 - val_loss: 0.3625\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 893us/step - loss: 0.3483 - val_loss: 0.3632\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 0.3474 - val_loss: 0.3631\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 920us/step - loss: 0.3473 - val_loss: 0.3627\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 953us/step - loss: 0.3466 - val_loss: 0.3656\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3482 - val_loss: 0.3604\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 916us/step - loss: 0.3459 - val_loss: 0.3606\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 0.3454 - val_loss: 0.3604\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 937us/step - loss: 0.3502 - val_loss: 0.3696\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 886us/step - loss: 0.3457 - val_loss: 0.3616\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 906us/step - loss: 0.3451 - val_loss: 0.3605\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 897us/step - loss: 0.3435 - val_loss: 0.3584\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 900us/step - loss: 0.3435 - val_loss: 0.3596\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.3447 - val_loss: 0.3600\n",
      "121/121 [==============================] - 0s 625us/step - loss: 0.3735\n",
      "Epoch 1/100\n",
      "  1/242 [..............................] - ETA: 35s - loss: 10.0618"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\ML_PATH\\my_env\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 1ms/step - loss: 1.6089 - val_loss: 0.6382\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 887us/step - loss: 0.6174 - val_loss: 0.5801\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 907us/step - loss: 0.5747 - val_loss: 0.5552\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 877us/step - loss: 0.5460 - val_loss: 0.5275\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 900us/step - loss: 0.5253 - val_loss: 0.5305\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 899us/step - loss: 0.5111 - val_loss: 0.4976\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 872us/step - loss: 0.4971 - val_loss: 0.4900\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 866us/step - loss: 0.4839 - val_loss: 0.4755\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 895us/step - loss: 0.4785 - val_loss: 0.4735\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 891us/step - loss: 0.4688 - val_loss: 0.4622\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.4623 - val_loss: 0.4575\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 873us/step - loss: 0.4544 - val_loss: 0.4508\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.4564 - val_loss: 0.4521\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 886us/step - loss: 0.4472 - val_loss: 0.4471\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 864us/step - loss: 0.4430 - val_loss: 0.4414\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 959us/step - loss: 0.4387 - val_loss: 0.4371\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.4360 - val_loss: 0.4348\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 908us/step - loss: 0.4337 - val_loss: 0.4363\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 935us/step - loss: 0.4302 - val_loss: 0.4313\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 921us/step - loss: 0.4277 - val_loss: 0.4312\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 975us/step - loss: 0.4260 - val_loss: 0.4289\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 963us/step - loss: 0.4241 - val_loss: 0.4261\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 880us/step - loss: 0.4212 - val_loss: 0.4247\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.4195 - val_loss: 0.4229\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 878us/step - loss: 0.4211 - val_loss: 0.4244\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 910us/step - loss: 0.4169 - val_loss: 0.4203\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 936us/step - loss: 0.4147 - val_loss: 0.4190\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 928us/step - loss: 0.4130 - val_loss: 0.4199\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 891us/step - loss: 0.4119 - val_loss: 0.4172\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 0.4104 - val_loss: 0.4144\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 906us/step - loss: 0.4111 - val_loss: 0.4184\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 981us/step - loss: 0.4088 - val_loss: 0.4151\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 0.4072 - val_loss: 0.4148\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4062 - val_loss: 0.4112\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 910us/step - loss: 0.4046 - val_loss: 0.4088\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 928us/step - loss: 0.4037 - val_loss: 0.4118\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 911us/step - loss: 0.4026 - val_loss: 0.4092\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 923us/step - loss: 0.4013 - val_loss: 0.4086\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 908us/step - loss: 0.4002 - val_loss: 0.4091\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 913us/step - loss: 0.4024 - val_loss: 0.4074\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 993us/step - loss: 0.4000 - val_loss: 0.4102\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 980us/step - loss: 0.3993 - val_loss: 0.4058\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 938us/step - loss: 0.3975 - val_loss: 0.4045\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 946us/step - loss: 0.3962 - val_loss: 0.4043\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 888us/step - loss: 0.3984 - val_loss: 0.4030\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 910us/step - loss: 0.4014 - val_loss: 0.4060\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 887us/step - loss: 0.3959 - val_loss: 0.4098\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 0.3945 - val_loss: 0.4023\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 891us/step - loss: 0.3943 - val_loss: 0.4004\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 864us/step - loss: 0.3929 - val_loss: 0.4000\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 908us/step - loss: 0.3953 - val_loss: 0.4036\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 885us/step - loss: 0.3909 - val_loss: 0.3983\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 919us/step - loss: 0.3906 - val_loss: 0.3993\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3899 - val_loss: 0.3978\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 885us/step - loss: 0.3934 - val_loss: 0.3966\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 936us/step - loss: 0.3992 - val_loss: 0.4015\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 921us/step - loss: 0.3952 - val_loss: 0.3977\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 904us/step - loss: 0.3896 - val_loss: 0.4078\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.3904 - val_loss: 0.3998\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 956us/step - loss: 0.3868 - val_loss: 0.3953\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 906us/step - loss: 0.3862 - val_loss: 0.3946\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 0.3863 - val_loss: 0.3968\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 964us/step - loss: 0.3855 - val_loss: 0.3950\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.3844 - val_loss: 0.3930\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.3839 - val_loss: 0.4042\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 932us/step - loss: 0.4067 - val_loss: 0.3959\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.3841 - val_loss: 0.3952\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 988us/step - loss: 0.3854 - val_loss: 0.3987\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3865 - val_loss: 0.3934\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 946us/step - loss: 0.3829 - val_loss: 0.3920\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 906us/step - loss: 0.3810 - val_loss: 0.3952\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.3815 - val_loss: 0.3912\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 941us/step - loss: 0.3811 - val_loss: 0.3919\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 908us/step - loss: 0.3832 - val_loss: 0.3924\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3807 - val_loss: 0.4006\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.3946 - val_loss: 0.3954\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 910us/step - loss: 0.3819 - val_loss: 0.3951\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 866us/step - loss: 0.3852 - val_loss: 0.3925\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 939us/step - loss: 0.3815 - val_loss: 0.3905\n",
      "Epoch 80/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 884us/step - loss: 0.3779 - val_loss: 0.3887\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 946us/step - loss: 0.3771 - val_loss: 0.3958\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 952us/step - loss: 0.3777 - val_loss: 0.3872\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 920us/step - loss: 0.3969 - val_loss: 0.3983\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 868us/step - loss: 0.3823 - val_loss: 0.4560\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 900us/step - loss: 0.3911 - val_loss: 0.4207\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 902us/step - loss: 0.3828 - val_loss: 0.3920\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 915us/step - loss: 0.3849 - val_loss: 0.4004\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 890us/step - loss: 0.3787 - val_loss: 0.3894\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 959us/step - loss: 0.3772 - val_loss: 0.3867\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 973us/step - loss: 0.3745 - val_loss: 0.3923\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 878us/step - loss: 0.3743 - val_loss: 0.3839\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 956us/step - loss: 0.3735 - val_loss: 0.3851\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 876us/step - loss: 0.3750 - val_loss: 0.3934\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 981us/step - loss: 0.4005 - val_loss: 0.3861\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 902us/step - loss: 0.3791 - val_loss: 0.4001\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 985us/step - loss: 0.4072 - val_loss: 0.3882\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 946us/step - loss: 0.3762 - val_loss: 0.3868\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 927us/step - loss: 0.3738 - val_loss: 0.3877\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 938us/step - loss: 0.3923 - val_loss: 0.3920\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 908us/step - loss: 0.3834 - val_loss: 0.3958\n",
      "121/121 [==============================] - 0s 613us/step - loss: 0.3877\n",
      "Epoch 1/100\n",
      "  1/242 [..............................] - ETA: 32s - loss: 4.3680"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\ML_PATH\\my_env\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 1ms/step - loss: 1.4856 - val_loss: 0.7529\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 948us/step - loss: 0.6996 - val_loss: 0.6575\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.6329 - val_loss: 0.6087\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 902us/step - loss: 0.5889 - val_loss: 0.5746\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.5590 - val_loss: 0.5472\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 934us/step - loss: 0.5339 - val_loss: 0.5255\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 902us/step - loss: 0.5170 - val_loss: 0.5138\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 878us/step - loss: 0.5032 - val_loss: 0.5068\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 976us/step - loss: 0.4926 - val_loss: 0.4975\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 899us/step - loss: 0.4873 - val_loss: 0.4908\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 950us/step - loss: 0.4802 - val_loss: 0.4940\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 868us/step - loss: 0.4758 - val_loss: 0.4835\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 906us/step - loss: 0.4711 - val_loss: 0.4836\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 966us/step - loss: 0.4697 - val_loss: 0.4780\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.4641 - val_loss: 0.4730\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 887us/step - loss: 0.4633 - val_loss: 0.4672\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 855us/step - loss: 0.4595 - val_loss: 0.4667\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 971us/step - loss: 0.4592 - val_loss: 0.4623\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 936us/step - loss: 0.4542 - val_loss: 0.4647\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 889us/step - loss: 0.4495 - val_loss: 0.4584\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4481 - val_loss: 0.4620\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 929us/step - loss: 0.4471 - val_loss: 0.4547\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 941us/step - loss: 0.4434 - val_loss: 0.4507\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 912us/step - loss: 0.4423 - val_loss: 0.4501\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 866us/step - loss: 0.4399 - val_loss: 0.4472\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 931us/step - loss: 0.4381 - val_loss: 0.4471\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 969us/step - loss: 0.4359 - val_loss: 0.4431\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 895us/step - loss: 0.4358 - val_loss: 0.4430\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 985us/step - loss: 0.4330 - val_loss: 0.4402\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 903us/step - loss: 0.4296 - val_loss: 0.4387\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 937us/step - loss: 0.4290 - val_loss: 0.4347\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 920us/step - loss: 0.4279 - val_loss: 0.4337\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 828us/step - loss: 0.4243 - val_loss: 0.4329\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 865us/step - loss: 0.4233 - val_loss: 0.4290\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 984us/step - loss: 0.4198 - val_loss: 0.4255\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 931us/step - loss: 0.4172 - val_loss: 0.4259\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.4146 - val_loss: 0.4208\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 978us/step - loss: 0.4116 - val_loss: 0.4156\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 939us/step - loss: 0.4095 - val_loss: 0.4131\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 919us/step - loss: 0.4060 - val_loss: 0.4118\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 914us/step - loss: 0.4058 - val_loss: 0.4093\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.4034 - val_loss: 0.4094\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 0.4011 - val_loss: 0.4072\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 922us/step - loss: 0.3997 - val_loss: 0.4065\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 913us/step - loss: 0.3977 - val_loss: 0.4020\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 907us/step - loss: 0.3973 - val_loss: 0.4045\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 912us/step - loss: 0.3957 - val_loss: 0.4001\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 874us/step - loss: 0.3936 - val_loss: 0.4005\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.3921 - val_loss: 0.3972\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.3911 - val_loss: 0.3954\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 885us/step - loss: 0.3900 - val_loss: 0.3959\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 946us/step - loss: 0.3873 - val_loss: 0.3951\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 888us/step - loss: 0.3870 - val_loss: 0.3931\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 0.3871 - val_loss: 0.3928\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 951us/step - loss: 0.3844 - val_loss: 0.3894\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 890us/step - loss: 0.3830 - val_loss: 0.3891\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 919us/step - loss: 0.3811 - val_loss: 0.3876\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 931us/step - loss: 0.3798 - val_loss: 0.3850\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 960us/step - loss: 0.3799 - val_loss: 0.3853\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 948us/step - loss: 0.3782 - val_loss: 0.3833\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 840us/step - loss: 0.3774 - val_loss: 0.3821\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 874us/step - loss: 0.3747 - val_loss: 0.3842\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 851us/step - loss: 0.3744 - val_loss: 0.3802\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3730 - val_loss: 0.3801\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 958us/step - loss: 0.3719 - val_loss: 0.3791\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3702 - val_loss: 0.3830\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3690 - val_loss: 0.3776\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3693 - val_loss: 0.3778\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3673 - val_loss: 0.3778\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3656 - val_loss: 0.3769\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 959us/step - loss: 0.3677 - val_loss: 0.3741\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 949us/step - loss: 0.3659 - val_loss: 0.3742\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.3652 - val_loss: 0.3757\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 879us/step - loss: 0.3623 - val_loss: 0.3729\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 921us/step - loss: 0.3610 - val_loss: 0.3728\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 929us/step - loss: 0.3639 - val_loss: 0.3714\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 900us/step - loss: 0.3609 - val_loss: 0.3711\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 975us/step - loss: 0.3597 - val_loss: 0.3689\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 885us/step - loss: 0.3582 - val_loss: 0.3698\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 906us/step - loss: 0.3581 - val_loss: 0.3720\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 851us/step - loss: 0.3598 - val_loss: 0.3685\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 968us/step - loss: 0.3585 - val_loss: 0.3677\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 924us/step - loss: 0.3569 - val_loss: 0.3673\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 847us/step - loss: 0.3585 - val_loss: 0.3668\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 839us/step - loss: 0.3566 - val_loss: 0.3661\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 911us/step - loss: 0.3539 - val_loss: 0.3680\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 925us/step - loss: 0.3547 - val_loss: 0.3665\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 898us/step - loss: 0.3525 - val_loss: 0.3675\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 899us/step - loss: 0.3539 - val_loss: 0.3646\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 930us/step - loss: 0.3568 - val_loss: 0.3679\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 918us/step - loss: 0.3566 - val_loss: 0.3688\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 876us/step - loss: 0.3573 - val_loss: 0.3647\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 941us/step - loss: 0.3505 - val_loss: 0.3649\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 834us/step - loss: 0.3514 - val_loss: 0.3632\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 846us/step - loss: 0.3495 - val_loss: 0.3614\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 886us/step - loss: 0.3489 - val_loss: 0.3636\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 952us/step - loss: 0.3483 - val_loss: 0.3650\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 930us/step - loss: 0.3477 - val_loss: 0.3629\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 915us/step - loss: 0.3491 - val_loss: 0.3621\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 910us/step - loss: 0.3530 - val_loss: 0.3607\n",
      "121/121 [==============================] - 0s 614us/step - loss: 0.3471\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\ML_PATH\\my_env\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 1ms/step - loss: 3.4583 - val_loss: 1.8692\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 1.3292 - val_loss: 0.9841\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 950us/step - loss: 0.8681 - val_loss: 0.7638\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.7158 - val_loss: 0.6710\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 965us/step - loss: 0.6551 - val_loss: 0.6378\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 933us/step - loss: 0.6281 - val_loss: 0.6168\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 941us/step - loss: 0.6096 - val_loss: 0.6013\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.5943 - val_loss: 0.5877\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 903us/step - loss: 0.5807 - val_loss: 0.5753\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 963us/step - loss: 0.5678 - val_loss: 0.5639\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 980us/step - loss: 0.5565 - val_loss: 0.5533\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 888us/step - loss: 0.5462 - val_loss: 0.5431\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 971us/step - loss: 0.5360 - val_loss: 0.5343\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 907us/step - loss: 0.5273 - val_loss: 0.5253\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 902us/step - loss: 0.5186 - val_loss: 0.5173\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 956us/step - loss: 0.5108 - val_loss: 0.5097\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 939us/step - loss: 0.5034 - val_loss: 0.5028\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 943us/step - loss: 0.4967 - val_loss: 0.4972\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 938us/step - loss: 0.4907 - val_loss: 0.4909\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.4851 - val_loss: 0.4864\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4799 - val_loss: 0.4814\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 902us/step - loss: 0.4751 - val_loss: 0.4767\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 907us/step - loss: 0.4708 - val_loss: 0.4729\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 878us/step - loss: 0.4666 - val_loss: 0.4684\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 877us/step - loss: 0.4631 - val_loss: 0.4656\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 891us/step - loss: 0.4592 - val_loss: 0.4620\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4564 - val_loss: 0.4592\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 976us/step - loss: 0.4527 - val_loss: 0.4554\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 920us/step - loss: 0.4502 - val_loss: 0.4534\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 893us/step - loss: 0.4469 - val_loss: 0.4503\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 922us/step - loss: 0.4439 - val_loss: 0.4481\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 914us/step - loss: 0.4409 - val_loss: 0.4453\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 974us/step - loss: 0.4395 - val_loss: 0.4432\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 867us/step - loss: 0.4362 - val_loss: 0.4401\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 955us/step - loss: 0.4343 - val_loss: 0.4388\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 965us/step - loss: 0.4314 - val_loss: 0.4359\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 943us/step - loss: 0.4291 - val_loss: 0.4338\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 907us/step - loss: 0.4268 - val_loss: 0.4316\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 963us/step - loss: 0.4245 - val_loss: 0.4301\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 963us/step - loss: 0.4224 - val_loss: 0.4277\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4201 - val_loss: 0.4252\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 995us/step - loss: 0.4180 - val_loss: 0.4250\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 946us/step - loss: 0.4164 - val_loss: 0.4222\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 968us/step - loss: 0.4143 - val_loss: 0.4198\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4122 - val_loss: 0.4178\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 951us/step - loss: 0.4103 - val_loss: 0.4155\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 910us/step - loss: 0.4087 - val_loss: 0.4137\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 869us/step - loss: 0.4062 - val_loss: 0.4115\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.4048 - val_loss: 0.4106\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 995us/step - loss: 0.4028 - val_loss: 0.4087\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 937us/step - loss: 0.4012 - val_loss: 0.4072\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 958us/step - loss: 0.3997 - val_loss: 0.4049\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3977 - val_loss: 0.4036\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 863us/step - loss: 0.3964 - val_loss: 0.4018\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 928us/step - loss: 0.3946 - val_loss: 0.4006\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 984us/step - loss: 0.3928 - val_loss: 0.4000\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.3918 - val_loss: 0.3977\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 874us/step - loss: 0.3904 - val_loss: 0.3969\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 921us/step - loss: 0.3889 - val_loss: 0.3960\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 883us/step - loss: 0.3874 - val_loss: 0.3946\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 913us/step - loss: 0.3862 - val_loss: 0.3929\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 992us/step - loss: 0.3845 - val_loss: 0.3923\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 987us/step - loss: 0.3832 - val_loss: 0.3906\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 983us/step - loss: 0.3820 - val_loss: 0.3893\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 906us/step - loss: 0.3809 - val_loss: 0.3886\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 950us/step - loss: 0.3796 - val_loss: 0.3890\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 882us/step - loss: 0.3782 - val_loss: 0.3863\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 967us/step - loss: 0.3773 - val_loss: 0.3860\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 917us/step - loss: 0.3762 - val_loss: 0.3852\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 876us/step - loss: 0.3751 - val_loss: 0.3833\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 906us/step - loss: 0.3736 - val_loss: 0.3843\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 980us/step - loss: 0.3728 - val_loss: 0.3824\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 918us/step - loss: 0.3718 - val_loss: 0.3818\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 897us/step - loss: 0.3709 - val_loss: 0.3807\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 910us/step - loss: 0.3696 - val_loss: 0.3795\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 969us/step - loss: 0.3686 - val_loss: 0.3789\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 934us/step - loss: 0.3680 - val_loss: 0.3776\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 983us/step - loss: 0.3670 - val_loss: 0.3790\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 902us/step - loss: 0.3658 - val_loss: 0.3772\n",
      "Epoch 80/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 895us/step - loss: 0.3650 - val_loss: 0.3757\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 873us/step - loss: 0.3641 - val_loss: 0.3747\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 946us/step - loss: 0.3634 - val_loss: 0.3745\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 902us/step - loss: 0.3623 - val_loss: 0.3735\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 963us/step - loss: 0.3615 - val_loss: 0.3737\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 903us/step - loss: 0.3606 - val_loss: 0.3720\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 895us/step - loss: 0.3598 - val_loss: 0.3719\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 928us/step - loss: 0.3592 - val_loss: 0.3710\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 924us/step - loss: 0.3582 - val_loss: 0.3705\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 963us/step - loss: 0.3576 - val_loss: 0.3708\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 970us/step - loss: 0.3566 - val_loss: 0.3717\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 898us/step - loss: 0.3561 - val_loss: 0.3690\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 955us/step - loss: 0.3553 - val_loss: 0.3688\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3545 - val_loss: 0.3680\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 904us/step - loss: 0.3542 - val_loss: 0.3678\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 929us/step - loss: 0.3532 - val_loss: 0.3677\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3527 - val_loss: 0.3679\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 976us/step - loss: 0.3518 - val_loss: 0.3664\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 955us/step - loss: 0.3516 - val_loss: 0.3666\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 981us/step - loss: 0.3505 - val_loss: 0.3659\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 934us/step - loss: 0.3502 - val_loss: 0.3649\n",
      "121/121 [==============================] - 0s 640us/step - loss: 0.3706\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\ML_PATH\\my_env\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 1ms/step - loss: 3.1709 - val_loss: 1.7390\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 887us/step - loss: 1.4341 - val_loss: 1.2225\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.1205 - val_loss: 0.9981\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 917us/step - loss: 0.9037 - val_loss: 0.8180\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 970us/step - loss: 0.7714 - val_loss: 0.7338\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 954us/step - loss: 0.7158 - val_loss: 0.6975\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 973us/step - loss: 0.6863 - val_loss: 0.6730\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 952us/step - loss: 0.6613 - val_loss: 0.6501\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 936us/step - loss: 0.6382 - val_loss: 0.6293\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 935us/step - loss: 0.6178 - val_loss: 0.6111\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 970us/step - loss: 0.6007 - val_loss: 0.5959\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 987us/step - loss: 0.5853 - val_loss: 0.5814\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 979us/step - loss: 0.5711 - val_loss: 0.5681\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 0.5574 - val_loss: 0.5552\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 943us/step - loss: 0.5443 - val_loss: 0.5425\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5317 - val_loss: 0.5307\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 991us/step - loss: 0.5201 - val_loss: 0.5200\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 915us/step - loss: 0.5091 - val_loss: 0.5101\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4995 - val_loss: 0.5012\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 912us/step - loss: 0.4906 - val_loss: 0.4929\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 912us/step - loss: 0.4831 - val_loss: 0.4862\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 966us/step - loss: 0.4763 - val_loss: 0.4802\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 932us/step - loss: 0.4702 - val_loss: 0.4748\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 953us/step - loss: 0.4649 - val_loss: 0.4694\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 947us/step - loss: 0.4599 - val_loss: 0.4653\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 940us/step - loss: 0.4559 - val_loss: 0.4612\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4519 - val_loss: 0.4574\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4482 - val_loss: 0.4541\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4451 - val_loss: 0.4513\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 958us/step - loss: 0.4422 - val_loss: 0.4485\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 960us/step - loss: 0.4394 - val_loss: 0.4457\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 973us/step - loss: 0.4368 - val_loss: 0.4436\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 907us/step - loss: 0.4342 - val_loss: 0.4409\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 959us/step - loss: 0.4320 - val_loss: 0.4391\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 968us/step - loss: 0.4300 - val_loss: 0.4368\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4277 - val_loss: 0.4347\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 967us/step - loss: 0.4259 - val_loss: 0.4326\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 982us/step - loss: 0.4241 - val_loss: 0.4307\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4222 - val_loss: 0.4297\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 999us/step - loss: 0.4201 - val_loss: 0.4280\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 942us/step - loss: 0.4186 - val_loss: 0.4258\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 940us/step - loss: 0.4169 - val_loss: 0.4247\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 891us/step - loss: 0.4154 - val_loss: 0.4230\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4135 - val_loss: 0.4210\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 918us/step - loss: 0.4118 - val_loss: 0.4195\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 919us/step - loss: 0.4104 - val_loss: 0.4183\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 932us/step - loss: 0.4087 - val_loss: 0.4162\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 972us/step - loss: 0.4074 - val_loss: 0.4148\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4058 - val_loss: 0.4140\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4042 - val_loss: 0.4133\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 957us/step - loss: 0.4030 - val_loss: 0.4114\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 945us/step - loss: 0.4013 - val_loss: 0.4100\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 925us/step - loss: 0.4003 - val_loss: 0.4096\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 956us/step - loss: 0.3988 - val_loss: 0.4078\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 977us/step - loss: 0.3982 - val_loss: 0.4070\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 943us/step - loss: 0.3967 - val_loss: 0.4069\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3955 - val_loss: 0.4051\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 994us/step - loss: 0.3944 - val_loss: 0.4037\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3935 - val_loss: 0.4024\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 978us/step - loss: 0.3915 - val_loss: 0.4012\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 954us/step - loss: 0.3908 - val_loss: 0.4021\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3902 - val_loss: 0.4000\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 933us/step - loss: 0.3888 - val_loss: 0.3983\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3876 - val_loss: 0.3976\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3871 - val_loss: 0.3978\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3858 - val_loss: 0.3965\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3849 - val_loss: 0.3962\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 993us/step - loss: 0.3840 - val_loss: 0.3954\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3829 - val_loss: 0.3942\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 996us/step - loss: 0.3821 - val_loss: 0.3945\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 987us/step - loss: 0.3815 - val_loss: 0.3923\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3804 - val_loss: 0.3918\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3796 - val_loss: 0.3914\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3785 - val_loss: 0.3903\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 973us/step - loss: 0.3784 - val_loss: 0.3896\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3770 - val_loss: 0.3907\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 997us/step - loss: 0.3764 - val_loss: 0.3906\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 989us/step - loss: 0.3755 - val_loss: 0.3893\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3750 - val_loss: 0.3895\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3739 - val_loss: 0.3874\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3732 - val_loss: 0.3875\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3725 - val_loss: 0.3857\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3722 - val_loss: 0.3852\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3712 - val_loss: 0.3877\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 986us/step - loss: 0.3708 - val_loss: 0.3851\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 982us/step - loss: 0.3701 - val_loss: 0.3835\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3691 - val_loss: 0.3844\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 973us/step - loss: 0.3689 - val_loss: 0.3835\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 976us/step - loss: 0.3679 - val_loss: 0.3831\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 995us/step - loss: 0.3674 - val_loss: 0.3837\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 951us/step - loss: 0.3667 - val_loss: 0.3837\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 986us/step - loss: 0.3665 - val_loss: 0.3806\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 975us/step - loss: 0.3658 - val_loss: 0.3812\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 954us/step - loss: 0.3653 - val_loss: 0.3808\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 995us/step - loss: 0.3644 - val_loss: 0.3809\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3638 - val_loss: 0.3815\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 969us/step - loss: 0.3627 - val_loss: 0.3797\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 983us/step - loss: 0.3629 - val_loss: 0.3799\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 960us/step - loss: 0.3621 - val_loss: 0.3795\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 976us/step - loss: 0.3617 - val_loss: 0.3797\n",
      "121/121 [==============================] - 0s 631us/step - loss: 0.3859\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\ML_PATH\\my_env\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 1ms/step - loss: 3.0412 - val_loss: 1.4952\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 924us/step - loss: 1.2845 - val_loss: 1.0214\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 951us/step - loss: 0.9067 - val_loss: 0.8187\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 943us/step - loss: 0.7612 - val_loss: 0.7226\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 948us/step - loss: 0.6977 - val_loss: 0.6793\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 933us/step - loss: 0.6644 - val_loss: 0.6532\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 953us/step - loss: 0.6410 - val_loss: 0.6329\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 960us/step - loss: 0.6217 - val_loss: 0.6155\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 961us/step - loss: 0.6049 - val_loss: 0.5997\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 956us/step - loss: 0.5898 - val_loss: 0.5855\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 977us/step - loss: 0.5761 - val_loss: 0.5726\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 998us/step - loss: 0.5635 - val_loss: 0.5609\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 967us/step - loss: 0.5516 - val_loss: 0.5498\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 956us/step - loss: 0.5413 - val_loss: 0.5399\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 955us/step - loss: 0.5315 - val_loss: 0.5313\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 873us/step - loss: 0.5223 - val_loss: 0.5228\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5143 - val_loss: 0.5146\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 899us/step - loss: 0.5068 - val_loss: 0.5075\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 965us/step - loss: 0.4999 - val_loss: 0.5008\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4932 - val_loss: 0.4952\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4873 - val_loss: 0.4897\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4814 - val_loss: 0.4843\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4759 - val_loss: 0.4791\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4713 - val_loss: 0.4748\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 933us/step - loss: 0.4667 - val_loss: 0.4707\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 990us/step - loss: 0.4622 - val_loss: 0.4666\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4586 - val_loss: 0.4631\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 928us/step - loss: 0.4547 - val_loss: 0.4596\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4513 - val_loss: 0.4562\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 967us/step - loss: 0.4479 - val_loss: 0.4543\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4447 - val_loss: 0.4499\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 969us/step - loss: 0.4417 - val_loss: 0.4478\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 972us/step - loss: 0.4391 - val_loss: 0.4444\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 973us/step - loss: 0.4363 - val_loss: 0.4418\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 904us/step - loss: 0.4338 - val_loss: 0.4400\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4312 - val_loss: 0.4383\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 974us/step - loss: 0.4289 - val_loss: 0.4350\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 983us/step - loss: 0.4265 - val_loss: 0.4328\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 992us/step - loss: 0.4244 - val_loss: 0.4311\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 978us/step - loss: 0.4223 - val_loss: 0.4287\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 981us/step - loss: 0.4204 - val_loss: 0.4268\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 972us/step - loss: 0.4182 - val_loss: 0.4269\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 966us/step - loss: 0.4163 - val_loss: 0.4238\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 994us/step - loss: 0.4143 - val_loss: 0.4214\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 992us/step - loss: 0.4125 - val_loss: 0.4199\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 960us/step - loss: 0.4106 - val_loss: 0.4181\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4091 - val_loss: 0.4172\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 927us/step - loss: 0.4073 - val_loss: 0.4156\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4059 - val_loss: 0.4149\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 907us/step - loss: 0.4042 - val_loss: 0.4130\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.4030 - val_loss: 0.4101\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 899us/step - loss: 0.4012 - val_loss: 0.4089\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 986us/step - loss: 0.4001 - val_loss: 0.4075\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 949us/step - loss: 0.3983 - val_loss: 0.4066\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 955us/step - loss: 0.3967 - val_loss: 0.4056\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3961 - val_loss: 0.4045\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 970us/step - loss: 0.3952 - val_loss: 0.4035\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 980us/step - loss: 0.3935 - val_loss: 0.4031\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 999us/step - loss: 0.3925 - val_loss: 0.4008\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3910 - val_loss: 0.4002\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 985us/step - loss: 0.3899 - val_loss: 0.3996\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 963us/step - loss: 0.3889 - val_loss: 0.3986\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 970us/step - loss: 0.3878 - val_loss: 0.3967\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 953us/step - loss: 0.3869 - val_loss: 0.3961\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3861 - val_loss: 0.3955\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 998us/step - loss: 0.3848 - val_loss: 0.3940\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 975us/step - loss: 0.3839 - val_loss: 0.3937\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 969us/step - loss: 0.3829 - val_loss: 0.3938\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 929us/step - loss: 0.3820 - val_loss: 0.3933\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 976us/step - loss: 0.3811 - val_loss: 0.3921\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 978us/step - loss: 0.3805 - val_loss: 0.3911\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 971us/step - loss: 0.3796 - val_loss: 0.3902\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3785 - val_loss: 0.3892\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 946us/step - loss: 0.3776 - val_loss: 0.3906\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 880us/step - loss: 0.3772 - val_loss: 0.3894\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3765 - val_loss: 0.3877\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3760 - val_loss: 0.3890\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 989us/step - loss: 0.3752 - val_loss: 0.3865\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 961us/step - loss: 0.3744 - val_loss: 0.3857\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 982us/step - loss: 0.3737 - val_loss: 0.3874\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3732 - val_loss: 0.3847\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 946us/step - loss: 0.3726 - val_loss: 0.3840\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 966us/step - loss: 0.3718 - val_loss: 0.3841\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 911us/step - loss: 0.3716 - val_loss: 0.3842\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 963us/step - loss: 0.3709 - val_loss: 0.3827\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 993us/step - loss: 0.3704 - val_loss: 0.3830\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 929us/step - loss: 0.3695 - val_loss: 0.3816\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 954us/step - loss: 0.3688 - val_loss: 0.3812\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 928us/step - loss: 0.3683 - val_loss: 0.3818\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 966us/step - loss: 0.3680 - val_loss: 0.3833\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 904us/step - loss: 0.3676 - val_loss: 0.3845\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 964us/step - loss: 0.3671 - val_loss: 0.3803\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 904us/step - loss: 0.3660 - val_loss: 0.3798\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 936us/step - loss: 0.3661 - val_loss: 0.3810\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 933us/step - loss: 0.3657 - val_loss: 0.3799\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 953us/step - loss: 0.3653 - val_loss: 0.3793\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 923us/step - loss: 0.3644 - val_loss: 0.3794\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 978us/step - loss: 0.3642 - val_loss: 0.3775\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 895us/step - loss: 0.3636 - val_loss: 0.3786\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 933us/step - loss: 0.3631 - val_loss: 0.3774\n",
      "121/121 [==============================] - 0s 642us/step - loss: 0.3710\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\ML_PATH\\my_env\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 1s 1ms/step - loss: 1.4527 - val_loss: 0.9245\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 999us/step - loss: 0.5667 - val_loss: 0.5548\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 946us/step - loss: 0.4434 - val_loss: 0.4051\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 876us/step - loss: 0.4053 - val_loss: 0.4327\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.3870 - val_loss: 0.3887\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 949us/step - loss: 0.3722 - val_loss: 0.3691\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 948us/step - loss: 0.3715 - val_loss: 0.4241\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 932us/step - loss: 0.4070 - val_loss: 0.4573\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 949us/step - loss: 0.5783 - val_loss: 0.4072\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 928us/step - loss: 0.4093 - val_loss: 0.3881\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 950us/step - loss: 0.4038 - val_loss: 0.3865\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.3681 - val_loss: 0.3578\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 816us/step - loss: 0.3647 - val_loss: 0.3554\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 996us/step - loss: 0.3477 - val_loss: 0.3656\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 922us/step - loss: 0.3526 - val_loss: 0.3503\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 897us/step - loss: 0.3405 - val_loss: 0.3665\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.3518 - val_loss: 0.3788\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 981us/step - loss: 0.3406 - val_loss: 0.3546\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 948us/step - loss: 0.3298 - val_loss: 0.3410\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 925us/step - loss: 0.3304 - val_loss: 0.3436\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 936us/step - loss: 0.3202 - val_loss: 0.3569\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 931us/step - loss: 0.3285 - val_loss: 0.3734\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 912us/step - loss: 0.3211 - val_loss: 0.3304\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 934us/step - loss: 0.3190 - val_loss: 0.3391\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 939us/step - loss: 0.3190 - val_loss: 0.3372\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 934us/step - loss: 0.3197 - val_loss: 0.3423\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 904us/step - loss: 0.3176 - val_loss: 0.3389\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 939us/step - loss: 0.3250 - val_loss: 0.3426\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 950us/step - loss: 0.3130 - val_loss: 0.3852\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 941us/step - loss: 0.3118 - val_loss: 0.3228\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 994us/step - loss: 0.3192 - val_loss: 0.3222\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 0.3075 - val_loss: 0.3194\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 936us/step - loss: 0.3100 - val_loss: 0.3790\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 871us/step - loss: 0.3017 - val_loss: 0.3578\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 904us/step - loss: 0.3078 - val_loss: 0.3311\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 918us/step - loss: 0.3066 - val_loss: 0.3205\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 893us/step - loss: 0.3052 - val_loss: 0.3453\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 914us/step - loss: 0.3072 - val_loss: 0.3253\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 966us/step - loss: 0.3137 - val_loss: 0.3222\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 987us/step - loss: 0.3076 - val_loss: 0.3935\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 882us/step - loss: 0.3050 - val_loss: 0.3192\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.3040 - val_loss: 0.3214\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 965us/step - loss: 0.2996 - val_loss: 0.3617\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 950us/step - loss: 0.2988 - val_loss: 0.4347\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 854us/step - loss: 0.2994 - val_loss: 0.3122\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2994 - val_loss: 0.3276\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 917us/step - loss: 0.3006 - val_loss: 0.3601\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 989us/step - loss: 0.3011 - val_loss: 0.3139\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 989us/step - loss: 0.2941 - val_loss: 0.3504\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 951us/step - loss: 0.2946 - val_loss: 0.3412\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 912us/step - loss: 0.3022 - val_loss: 0.3178\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 936us/step - loss: 0.2955 - val_loss: 0.3203\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 904us/step - loss: 0.2953 - val_loss: 0.3288\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.2929 - val_loss: 0.3167\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 932us/step - loss: 0.2921 - val_loss: 0.3329\n",
      "121/121 [==============================] - 0s 603us/step - loss: 0.3234\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\ML_PATH\\my_env\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 1ms/step - loss: 1.1960 - val_loss: 0.6064\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 913us/step - loss: 0.5181 - val_loss: 0.4865\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4395 - val_loss: 0.4112\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 939us/step - loss: 0.4094 - val_loss: 0.4024\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 946us/step - loss: 0.4038 - val_loss: 0.4077\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3878 - val_loss: 0.3699\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3834 - val_loss: 0.3723\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 953us/step - loss: 0.3613 - val_loss: 0.3802\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 971us/step - loss: 0.3676 - val_loss: 0.3854\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 969us/step - loss: 0.3636 - val_loss: 0.3834\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 935us/step - loss: 0.3588 - val_loss: 0.4244\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 925us/step - loss: 0.3504 - val_loss: 0.3652\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 978us/step - loss: 0.3564 - val_loss: 0.3411\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 937us/step - loss: 0.3478 - val_loss: 0.3739\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3430 - val_loss: 0.3368\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 861us/step - loss: 0.3496 - val_loss: 0.3538\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 985us/step - loss: 0.3335 - val_loss: 0.3369\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 960us/step - loss: 0.3339 - val_loss: 0.3457\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 987us/step - loss: 0.3433 - val_loss: 0.3445\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3410 - val_loss: 0.3855\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 910us/step - loss: 0.3335 - val_loss: 0.3438\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3263 - val_loss: 0.3345\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 929us/step - loss: 0.3263 - val_loss: 0.3837\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3201 - val_loss: 0.3290\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 997us/step - loss: 0.3216 - val_loss: 0.3268\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 936us/step - loss: 0.3546 - val_loss: 0.3987\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 970us/step - loss: 0.3521 - val_loss: 0.3591\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 966us/step - loss: 0.3274 - val_loss: 0.3301\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 963us/step - loss: 0.3204 - val_loss: 0.3296\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3210 - val_loss: 0.3478\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 952us/step - loss: 0.3154 - val_loss: 0.3278\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3122 - val_loss: 0.3326\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 956us/step - loss: 0.3148 - val_loss: 0.3259\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3083 - val_loss: 0.3285\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 958us/step - loss: 0.3099 - val_loss: 0.3529\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 966us/step - loss: 0.3091 - val_loss: 0.3682\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 973us/step - loss: 0.3051 - val_loss: 0.3424\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 981us/step - loss: 0.3063 - val_loss: 0.3171\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 974us/step - loss: 0.3088 - val_loss: 0.3242\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 935us/step - loss: 0.3065 - val_loss: 0.3853\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 977us/step - loss: 0.3112 - val_loss: 0.3311\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 973us/step - loss: 0.3013 - val_loss: 0.3585\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 920us/step - loss: 0.3033 - val_loss: 0.3198\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3066 - val_loss: 0.3265\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3004 - val_loss: 0.3368\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 967us/step - loss: 0.2999 - val_loss: 0.3343\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 951us/step - loss: 0.3022 - val_loss: 0.3243\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3010 - val_loss: 0.3907\n",
      "121/121 [==============================] - 0s 644us/step - loss: 0.3694\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\ML_PATH\\my_env\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 1s 1ms/step - loss: 0.8256 - val_loss: 0.4854\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 937us/step - loss: 0.4477 - val_loss: 0.4342\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 953us/step - loss: 0.4141 - val_loss: 0.3916\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 952us/step - loss: 0.3947 - val_loss: 0.3853\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3835 - val_loss: 0.3803\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 960us/step - loss: 0.3762 - val_loss: 0.3763\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3693 - val_loss: 0.3716\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3687 - val_loss: 0.4136\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 973us/step - loss: 0.3634 - val_loss: 0.3711\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 940us/step - loss: 0.3551 - val_loss: 0.3623\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 929us/step - loss: 0.3537 - val_loss: 0.3630\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 964us/step - loss: 0.3491 - val_loss: 0.3759\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3490 - val_loss: 0.3623\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3496 - val_loss: 0.3761\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 981us/step - loss: 0.3464 - val_loss: 0.3501\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 928us/step - loss: 0.3385 - val_loss: 0.3679\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 971us/step - loss: 0.3455 - val_loss: 0.3565\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 963us/step - loss: 0.3368 - val_loss: 0.3758\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 980us/step - loss: 0.3330 - val_loss: 0.3707\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 969us/step - loss: 0.3322 - val_loss: 0.3479\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 964us/step - loss: 0.3288 - val_loss: 0.3338\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3399 - val_loss: 0.4013\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 981us/step - loss: 0.3337 - val_loss: 0.3354\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 955us/step - loss: 0.3318 - val_loss: 0.3428\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 987us/step - loss: 0.3273 - val_loss: 0.3824\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 939us/step - loss: 0.3248 - val_loss: 0.3485\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 951us/step - loss: 0.3221 - val_loss: 0.4646\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 898us/step - loss: 0.3205 - val_loss: 0.3394\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3332 - val_loss: 0.3536\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 967us/step - loss: 0.3169 - val_loss: 0.3522\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 993us/step - loss: 0.3183 - val_loss: 0.3478\n",
      "121/121 [==============================] - 0s 596us/step - loss: 0.3305\n",
      "Epoch 1/100\n",
      "  1/242 [..............................] - ETA: 35s - loss: 7.4591"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\ML_PATH\\my_env\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 875us/step - loss: nan - val_loss: nan\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 883us/step - loss: nan - val_loss: nan\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 830us/step - loss: nan - val_loss: nan\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 832us/step - loss: nan - val_loss: nan\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 823us/step - loss: nan - val_loss: nan\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 961us/step - loss: nan - val_loss: nan\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 874us/step - loss: nan - val_loss: nan\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 887us/step - loss: nan - val_loss: nan\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 829us/step - loss: nan - val_loss: nan\n",
      "121/121 [==============================] - 0s 608us/step - loss: nan\n",
      "Epoch 1/100\n",
      "  1/242 [..............................] - ETA: 34s - loss: 7.0031"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\ML_PATH\\my_env\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7740 - val_loss: 0.6565\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 842us/step - loss: 1.5200 - val_loss: 0.4521\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.4494 - val_loss: 0.4335\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 0.4228 - val_loss: 0.4014\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 844us/step - loss: 0.4080 - val_loss: 0.3978\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 940us/step - loss: 0.3923 - val_loss: 0.4294\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 862us/step - loss: 0.4964 - val_loss: 0.4146\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 953us/step - loss: 0.6006 - val_loss: 0.4607\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 0.4100 - val_loss: 0.4031\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.4095 - val_loss: 0.4254\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 900us/step - loss: 2.1596 - val_loss: 0.5086\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 866us/step - loss: 0.5789 - val_loss: 0.4799\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 895us/step - loss: 0.4212 - val_loss: 0.4157\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 939us/step - loss: 0.3913 - val_loss: 0.3922\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 889us/step - loss: 0.3758 - val_loss: 0.3924\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.3777 - val_loss: 0.3758\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 865us/step - loss: 0.3679 - val_loss: 0.3759\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 939us/step - loss: 0.3602 - val_loss: 0.3701\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 963us/step - loss: 0.3520 - val_loss: 0.3696\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 885us/step - loss: 0.3465 - val_loss: 0.3581\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 965us/step - loss: 0.3429 - val_loss: 0.3766\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3394 - val_loss: 0.3584\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3369 - val_loss: 0.3562\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3394 - val_loss: 0.3679\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3379 - val_loss: 0.3581\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3382 - val_loss: 0.3470\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 881us/step - loss: 0.3294 - val_loss: 0.3718\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 952us/step - loss: 0.3270 - val_loss: 0.3487\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 902us/step - loss: 0.3242 - val_loss: 0.3427\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 0.3399 - val_loss: 0.3461\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 868us/step - loss: 0.3289 - val_loss: 0.3536\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 918us/step - loss: 0.4266 - val_loss: 0.3569\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 910us/step - loss: 0.3377 - val_loss: 0.3482\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 862us/step - loss: 0.3278 - val_loss: 0.3512\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 922us/step - loss: 0.3264 - val_loss: 0.3509\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 891us/step - loss: 0.3253 - val_loss: 0.3614\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 859us/step - loss: 0.3219 - val_loss: 0.3541\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 878us/step - loss: 0.3200 - val_loss: 0.3528\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 814us/step - loss: 0.3198 - val_loss: 0.3380\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 965us/step - loss: 0.3245 - val_loss: 0.3438\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 897us/step - loss: 0.3169 - val_loss: 0.3357\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 897us/step - loss: 0.3171 - val_loss: 0.3406\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 886us/step - loss: 0.3144 - val_loss: 0.3429\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 864us/step - loss: 0.3130 - val_loss: 0.3392\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 913us/step - loss: 0.3154 - val_loss: 0.3965\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 969us/step - loss: 0.3183 - val_loss: 0.3495\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3993 - val_loss: 0.3389\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 839us/step - loss: 0.3190 - val_loss: 0.3460\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.3178 - val_loss: 0.3432\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 882us/step - loss: 0.3136 - val_loss: 0.3416\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 921us/step - loss: 0.3137 - val_loss: 0.3374\n",
      "121/121 [==============================] - 0s 619us/step - loss: 0.3252\n",
      "Epoch 1/100\n",
      "  1/242 [..............................] - ETA: 32s - loss: 5.9294"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\ML_PATH\\my_env\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 1ms/step - loss: 0.9090 - val_loss: 1.1302\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 938us/step - loss: nan - val_loss: nan\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: nan - val_loss: nan\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 897us/step - loss: nan - val_loss: nan\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 855us/step - loss: nan - val_loss: nan\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 935us/step - loss: nan - val_loss: nan\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 925us/step - loss: nan - val_loss: nan\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 865us/step - loss: nan - val_loss: nan\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 951us/step - loss: nan - val_loss: nan\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 875us/step - loss: nan - val_loss: nan\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 860us/step - loss: nan - val_loss: nan\n",
      "121/121 [==============================] - 0s 603us/step - loss: nan\n",
      "Epoch 1/100\n",
      "  1/242 [..............................] - ETA: 38s - loss: 4.5229"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\ML_PATH\\my_env\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 1ms/step - loss: 2.5138 - val_loss: 0.9518\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 908us/step - loss: 0.8288 - val_loss: 0.7025\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 902us/step - loss: 0.6635 - val_loss: 0.6397\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 915us/step - loss: 0.6192 - val_loss: 0.6060\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 951us/step - loss: 0.5895 - val_loss: 0.5816\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 902us/step - loss: 0.5641 - val_loss: 0.5589\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 986us/step - loss: 0.5433 - val_loss: 0.5379\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 890us/step - loss: 0.5238 - val_loss: 0.5205\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 940us/step - loss: 0.5071 - val_loss: 0.5057\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 923us/step - loss: 0.4919 - val_loss: 0.4910\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 990us/step - loss: 0.4786 - val_loss: 0.4780\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 939us/step - loss: 0.4667 - val_loss: 0.4680\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 954us/step - loss: 0.4568 - val_loss: 0.4601\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 911us/step - loss: 0.4487 - val_loss: 0.4519\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 895us/step - loss: 0.4412 - val_loss: 0.4466\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 971us/step - loss: 0.4355 - val_loss: 0.4402\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 921us/step - loss: 0.4295 - val_loss: 0.4355\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 955us/step - loss: 0.4246 - val_loss: 0.4306\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 952us/step - loss: 0.4200 - val_loss: 0.4270\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 954us/step - loss: 0.4155 - val_loss: 0.4237\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 904us/step - loss: 0.4119 - val_loss: 0.4198\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 962us/step - loss: 0.4084 - val_loss: 0.4165\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 914us/step - loss: 0.4050 - val_loss: 0.4132\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 897us/step - loss: 0.4017 - val_loss: 0.4124\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 938us/step - loss: 0.3989 - val_loss: 0.4079\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 948us/step - loss: 0.3962 - val_loss: 0.4054\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 916us/step - loss: 0.3931 - val_loss: 0.4027\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 911us/step - loss: 0.3901 - val_loss: 0.4006\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3881 - val_loss: 0.3991\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 920us/step - loss: 0.3856 - val_loss: 0.3959\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.3830 - val_loss: 0.3953\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 879us/step - loss: 0.3808 - val_loss: 0.3926\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 940us/step - loss: 0.3789 - val_loss: 0.3908\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 985us/step - loss: 0.3769 - val_loss: 0.3887\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 910us/step - loss: 0.3751 - val_loss: 0.3870\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 939us/step - loss: 0.3730 - val_loss: 0.3860\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 975us/step - loss: 0.3714 - val_loss: 0.3850\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 904us/step - loss: 0.3694 - val_loss: 0.3836\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 891us/step - loss: 0.3676 - val_loss: 0.3813\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 964us/step - loss: 0.3661 - val_loss: 0.3819\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 959us/step - loss: 0.3644 - val_loss: 0.3803\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.3633 - val_loss: 0.3770\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.3623 - val_loss: 0.3758\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.3607 - val_loss: 0.3760\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 913us/step - loss: 0.3590 - val_loss: 0.3768\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3581 - val_loss: 0.3734\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3565 - val_loss: 0.3729\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 911us/step - loss: 0.3554 - val_loss: 0.3716\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 912us/step - loss: 0.3545 - val_loss: 0.3708\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 903us/step - loss: 0.3534 - val_loss: 0.3723\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 974us/step - loss: 0.3523 - val_loss: 0.3733\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 963us/step - loss: 0.3511 - val_loss: 0.3683\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 918us/step - loss: 0.3499 - val_loss: 0.3684\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 948us/step - loss: 0.3491 - val_loss: 0.3669\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 931us/step - loss: 0.3480 - val_loss: 0.3686\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.3475 - val_loss: 0.3651\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 923us/step - loss: 0.3461 - val_loss: 0.3661\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 888us/step - loss: 0.3452 - val_loss: 0.3636\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 992us/step - loss: 0.3447 - val_loss: 0.3646\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 919us/step - loss: 0.3434 - val_loss: 0.3664\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 935us/step - loss: 0.3432 - val_loss: 0.3640\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 953us/step - loss: 0.3418 - val_loss: 0.3619\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3411 - val_loss: 0.3622\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 939us/step - loss: 0.3401 - val_loss: 0.3619\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 935us/step - loss: 0.3393 - val_loss: 0.3626\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 951us/step - loss: 0.3387 - val_loss: 0.3603\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 969us/step - loss: 0.3378 - val_loss: 0.3589\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 979us/step - loss: 0.3370 - val_loss: 0.3591\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 954us/step - loss: 0.3367 - val_loss: 0.3588\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 0.3354 - val_loss: 0.3597\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 898us/step - loss: 0.3348 - val_loss: 0.3650\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 995us/step - loss: 0.3348 - val_loss: 0.3574\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 911us/step - loss: 0.3332 - val_loss: 0.3582\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 900us/step - loss: 0.3328 - val_loss: 0.3578\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3322 - val_loss: 0.3545\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3317 - val_loss: 0.3544\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 978us/step - loss: 0.3313 - val_loss: 0.3565\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 898us/step - loss: 0.3307 - val_loss: 0.3560\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 952us/step - loss: 0.3298 - val_loss: 0.3540\n",
      "Epoch 80/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 877us/step - loss: 0.3296 - val_loss: 0.3546\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 872us/step - loss: 0.3281 - val_loss: 0.3531\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 940us/step - loss: 0.3277 - val_loss: 0.3538\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 957us/step - loss: 0.3274 - val_loss: 0.3580\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3274 - val_loss: 0.3530\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 935us/step - loss: 0.3266 - val_loss: 0.3548\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 999us/step - loss: 0.3257 - val_loss: 0.3510\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 904us/step - loss: 0.3248 - val_loss: 0.3598\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 903us/step - loss: 0.3248 - val_loss: 0.3531\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 946us/step - loss: 0.3242 - val_loss: 0.3523\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 969us/step - loss: 0.3243 - val_loss: 0.3494\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 965us/step - loss: 0.3232 - val_loss: 0.3497\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 926us/step - loss: 0.3226 - val_loss: 0.3499\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 970us/step - loss: 0.3223 - val_loss: 0.3524\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 950us/step - loss: 0.3213 - val_loss: 0.3496\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 923us/step - loss: 0.3213 - val_loss: 0.3495\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3204 - val_loss: 0.3485\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 966us/step - loss: 0.3201 - val_loss: 0.3483\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 974us/step - loss: 0.3191 - val_loss: 0.3491\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 991us/step - loss: 0.3192 - val_loss: 0.3507\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 995us/step - loss: 0.3188 - val_loss: 0.3456\n",
      "121/121 [==============================] - 0s 630us/step - loss: 0.3555\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\ML_PATH\\my_env\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 1ms/step - loss: 2.2068 - val_loss: 0.9451\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 903us/step - loss: 0.8029 - val_loss: 0.7067\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 962us/step - loss: 0.6721 - val_loss: 0.6589\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 913us/step - loss: 0.6341 - val_loss: 0.6263\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 958us/step - loss: 0.6050 - val_loss: 0.6003\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 956us/step - loss: 0.5809 - val_loss: 0.5765\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 970us/step - loss: 0.5590 - val_loss: 0.5565\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 908us/step - loss: 0.5402 - val_loss: 0.5387\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 944us/step - loss: 0.5238 - val_loss: 0.5234\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 903us/step - loss: 0.5093 - val_loss: 0.5087\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 932us/step - loss: 0.4966 - val_loss: 0.4964\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 921us/step - loss: 0.4856 - val_loss: 0.4861\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 941us/step - loss: 0.4756 - val_loss: 0.4771\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 970us/step - loss: 0.4676 - val_loss: 0.4680\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 900us/step - loss: 0.4602 - val_loss: 0.4602\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 912us/step - loss: 0.4536 - val_loss: 0.4545\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 920us/step - loss: 0.4477 - val_loss: 0.4477\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 885us/step - loss: 0.4424 - val_loss: 0.4432\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 949us/step - loss: 0.4378 - val_loss: 0.4374\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 938us/step - loss: 0.4334 - val_loss: 0.4335\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 952us/step - loss: 0.4295 - val_loss: 0.4290\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 914us/step - loss: 0.4259 - val_loss: 0.4252\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 916us/step - loss: 0.4223 - val_loss: 0.4210\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 920us/step - loss: 0.4190 - val_loss: 0.4185\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 927us/step - loss: 0.4155 - val_loss: 0.4151\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 951us/step - loss: 0.4127 - val_loss: 0.4120\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 873us/step - loss: 0.4093 - val_loss: 0.4090\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 893us/step - loss: 0.4061 - val_loss: 0.4079\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 926us/step - loss: 0.4033 - val_loss: 0.4043\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 952us/step - loss: 0.4005 - val_loss: 0.4029\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 933us/step - loss: 0.3978 - val_loss: 0.3992\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 912us/step - loss: 0.3952 - val_loss: 0.3972\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 985us/step - loss: 0.3927 - val_loss: 0.3949\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.3902 - val_loss: 0.3924\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 918us/step - loss: 0.3877 - val_loss: 0.3899\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 883us/step - loss: 0.3852 - val_loss: 0.3897\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 941us/step - loss: 0.3833 - val_loss: 0.3864\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 914us/step - loss: 0.3812 - val_loss: 0.3848\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 967us/step - loss: 0.3796 - val_loss: 0.3850\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 938us/step - loss: 0.3777 - val_loss: 0.3820\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 929us/step - loss: 0.3758 - val_loss: 0.3806\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 967us/step - loss: 0.3739 - val_loss: 0.3791\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 974us/step - loss: 0.3726 - val_loss: 0.3781\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 913us/step - loss: 0.3704 - val_loss: 0.3775\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 900us/step - loss: 0.3691 - val_loss: 0.3772\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 879us/step - loss: 0.3678 - val_loss: 0.3740\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 958us/step - loss: 0.3666 - val_loss: 0.3747\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 960us/step - loss: 0.3651 - val_loss: 0.3733\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 973us/step - loss: 0.3636 - val_loss: 0.3715\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 861us/step - loss: 0.3626 - val_loss: 0.3701\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 911us/step - loss: 0.3607 - val_loss: 0.3743\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 958us/step - loss: 0.3601 - val_loss: 0.3678\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 940us/step - loss: 0.3590 - val_loss: 0.3679\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 946us/step - loss: 0.3581 - val_loss: 0.3669\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 914us/step - loss: 0.3572 - val_loss: 0.3658\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 893us/step - loss: 0.3558 - val_loss: 0.3661\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 935us/step - loss: 0.3550 - val_loss: 0.3663\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 959us/step - loss: 0.3541 - val_loss: 0.3649\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 902us/step - loss: 0.3526 - val_loss: 0.3650\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 941us/step - loss: 0.3522 - val_loss: 0.3664\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 867us/step - loss: 0.3513 - val_loss: 0.3631\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 981us/step - loss: 0.3506 - val_loss: 0.3621\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 931us/step - loss: 0.3494 - val_loss: 0.3629\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 941us/step - loss: 0.3483 - val_loss: 0.3640\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 1000us/step - loss: 0.3484 - val_loss: 0.3608\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 889us/step - loss: 0.3469 - val_loss: 0.3684\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 951us/step - loss: 0.3472 - val_loss: 0.3614\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 918us/step - loss: 0.3460 - val_loss: 0.3626\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 971us/step - loss: 0.3456 - val_loss: 0.3588\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 907us/step - loss: 0.3446 - val_loss: 0.3621\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 951us/step - loss: 0.3442 - val_loss: 0.3598\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 915us/step - loss: 0.3432 - val_loss: 0.3594\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 903us/step - loss: 0.3433 - val_loss: 0.3576\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 976us/step - loss: 0.3422 - val_loss: 0.3580\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 921us/step - loss: 0.3415 - val_loss: 0.3614\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 895us/step - loss: 0.3407 - val_loss: 0.3609\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 935us/step - loss: 0.3408 - val_loss: 0.3610\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 916us/step - loss: 0.3407 - val_loss: 0.3564\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3395 - val_loss: 0.3577\n",
      "Epoch 80/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 902us/step - loss: 0.3387 - val_loss: 0.3567\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 898us/step - loss: 0.3383 - val_loss: 0.3588\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 833us/step - loss: 0.3381 - val_loss: 0.3555\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 937us/step - loss: 0.3371 - val_loss: 0.3557\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.3373 - val_loss: 0.3577\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 953us/step - loss: 0.3370 - val_loss: 0.3550\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 889us/step - loss: 0.3363 - val_loss: 0.3543\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 897us/step - loss: 0.3360 - val_loss: 0.3545\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 916us/step - loss: 0.3353 - val_loss: 0.3565\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 962us/step - loss: 0.3350 - val_loss: 0.3531\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 943us/step - loss: 0.3346 - val_loss: 0.3535\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 890us/step - loss: 0.3343 - val_loss: 0.3539\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 937us/step - loss: 0.3339 - val_loss: 0.3533\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 978us/step - loss: 0.3331 - val_loss: 0.3542\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 913us/step - loss: 0.3328 - val_loss: 0.3528\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 955us/step - loss: 0.3327 - val_loss: 0.3555\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 929us/step - loss: 0.3327 - val_loss: 0.3536\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 950us/step - loss: 0.3316 - val_loss: 0.3538\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 877us/step - loss: 0.3311 - val_loss: 0.3546\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.3307 - val_loss: 0.3516\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 977us/step - loss: 0.3296 - val_loss: 0.3546\n",
      "121/121 [==============================] - 0s 610us/step - loss: 0.3554\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\ML_PATH\\my_env\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 1ms/step - loss: 3.0675 - val_loss: 1.6564\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 924us/step - loss: 1.3837 - val_loss: 1.3138\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 910us/step - loss: 1.2575 - val_loss: 1.2353\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 930us/step - loss: 1.1180 - val_loss: 1.0115\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 988us/step - loss: 0.8594 - val_loss: 0.7502\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 930us/step - loss: 0.6948 - val_loss: 0.6657\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 983us/step - loss: 0.6438 - val_loss: 0.6313\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 911us/step - loss: 0.6136 - val_loss: 0.6055\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 930us/step - loss: 0.5879 - val_loss: 0.5809\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 985us/step - loss: 0.5647 - val_loss: 0.5594\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 978us/step - loss: 0.5438 - val_loss: 0.5397\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 858us/step - loss: 0.5249 - val_loss: 0.5220\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 967us/step - loss: 0.5091 - val_loss: 0.5060\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 933us/step - loss: 0.4946 - val_loss: 0.4924\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 925us/step - loss: 0.4829 - val_loss: 0.4803\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4723 - val_loss: 0.4711\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4634 - val_loss: 0.4652\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4556 - val_loss: 0.4578\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4501 - val_loss: 0.4532\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4451 - val_loss: 0.4467\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4400 - val_loss: 0.4432\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 993us/step - loss: 0.4362 - val_loss: 0.4376\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 913us/step - loss: 0.4317 - val_loss: 0.4343\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 947us/step - loss: 0.4286 - val_loss: 0.4302\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 939us/step - loss: 0.4244 - val_loss: 0.4283\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4209 - val_loss: 0.4254\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 895us/step - loss: 0.4178 - val_loss: 0.4205\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 900us/step - loss: 0.4146 - val_loss: 0.4187\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 964us/step - loss: 0.4115 - val_loss: 0.4170\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 935us/step - loss: 0.4084 - val_loss: 0.4110\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 915us/step - loss: 0.4056 - val_loss: 0.4111\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 924us/step - loss: 0.4025 - val_loss: 0.4061\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 906us/step - loss: 0.4006 - val_loss: 0.4040\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 897us/step - loss: 0.3974 - val_loss: 0.4028\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 963us/step - loss: 0.3952 - val_loss: 0.3993\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 907us/step - loss: 0.3931 - val_loss: 0.3965\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 939us/step - loss: 0.3898 - val_loss: 0.3971\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 882us/step - loss: 0.3874 - val_loss: 0.3930\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 928us/step - loss: 0.3850 - val_loss: 0.3906\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.3837 - val_loss: 0.3897\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 900us/step - loss: 0.3818 - val_loss: 0.3877\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.3795 - val_loss: 0.3859\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 970us/step - loss: 0.3779 - val_loss: 0.3851\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 951us/step - loss: 0.3759 - val_loss: 0.3829\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 978us/step - loss: 0.3745 - val_loss: 0.3820\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 982us/step - loss: 0.3722 - val_loss: 0.3804\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 951us/step - loss: 0.3712 - val_loss: 0.3780\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 987us/step - loss: 0.3693 - val_loss: 0.3789\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 1000us/step - loss: 0.3672 - val_loss: 0.3767\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 0.3672 - val_loss: 0.3770\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 921us/step - loss: 0.3656 - val_loss: 0.3730\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 941us/step - loss: 0.3640 - val_loss: 0.3724\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 939us/step - loss: 0.3629 - val_loss: 0.3730\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 968us/step - loss: 0.3615 - val_loss: 0.3703\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 957us/step - loss: 0.3610 - val_loss: 0.3686\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 981us/step - loss: 0.3602 - val_loss: 0.3680\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 938us/step - loss: 0.3592 - val_loss: 0.3685\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 956us/step - loss: 0.3572 - val_loss: 0.3673\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 965us/step - loss: 0.3555 - val_loss: 0.3666\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 966us/step - loss: 0.3548 - val_loss: 0.3656\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 959us/step - loss: 0.3544 - val_loss: 0.3642\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 919us/step - loss: 0.3539 - val_loss: 0.3626\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 925us/step - loss: 0.3524 - val_loss: 0.3646\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 971us/step - loss: 0.3516 - val_loss: 0.3661\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 900us/step - loss: 0.3511 - val_loss: 0.3652\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 0.3498 - val_loss: 0.3651\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 973us/step - loss: 0.3495 - val_loss: 0.3613\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 913us/step - loss: 0.3477 - val_loss: 0.3605\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 890us/step - loss: 0.3472 - val_loss: 0.3620\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 976us/step - loss: 0.3466 - val_loss: 0.3581\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 926us/step - loss: 0.3464 - val_loss: 0.3602\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 938us/step - loss: 0.3455 - val_loss: 0.3593\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 963us/step - loss: 0.3436 - val_loss: 0.3610\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 989us/step - loss: 0.3436 - val_loss: 0.3606\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 895us/step - loss: 0.3417 - val_loss: 0.3609\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 936us/step - loss: 0.3429 - val_loss: 0.3584\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 936us/step - loss: 0.3428 - val_loss: 0.3591\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 944us/step - loss: 0.3415 - val_loss: 0.3588\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 923us/step - loss: 0.3421 - val_loss: 0.3607\n",
      "Epoch 80/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 918us/step - loss: 0.3405 - val_loss: 0.3582\n",
      "121/121 [==============================] - 0s 611us/step - loss: 0.3449\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\ML_PATH\\my_env\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 1ms/step - loss: 4.2034 - val_loss: 3.0188\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 987us/step - loss: 2.3446 - val_loss: 1.7541\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 950us/step - loss: 1.5433 - val_loss: 1.3712\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 963us/step - loss: 1.3567 - val_loss: 1.3150\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 941us/step - loss: 1.3289 - val_loss: 1.3049\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3202 - val_loss: 1.2977\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 953us/step - loss: 1.3120 - val_loss: 1.2885\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 1.3011 - val_loss: 1.2759\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 911us/step - loss: 1.2861 - val_loss: 1.2581\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 946us/step - loss: 1.2650 - val_loss: 1.2329\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 911us/step - loss: 1.2356 - val_loss: 1.1985\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 964us/step - loss: 1.1958 - val_loss: 1.1526\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 978us/step - loss: 1.1430 - val_loss: 1.0932\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 939us/step - loss: 1.0767 - val_loss: 1.0220\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 931us/step - loss: 1.0006 - val_loss: 0.9447\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 886us/step - loss: 0.9221 - val_loss: 0.8709\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 955us/step - loss: 0.8499 - val_loss: 0.8051\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 976us/step - loss: 0.7861 - val_loss: 0.7495\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 951us/step - loss: 0.7314 - val_loss: 0.7027\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6873 - val_loss: 0.6667\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 977us/step - loss: 0.6546 - val_loss: 0.6407\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 964us/step - loss: 0.6307 - val_loss: 0.6219\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 955us/step - loss: 0.6124 - val_loss: 0.6071\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 945us/step - loss: 0.5977 - val_loss: 0.5943\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 940us/step - loss: 0.5847 - val_loss: 0.5829\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 918us/step - loss: 0.5730 - val_loss: 0.5722\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 904us/step - loss: 0.5618 - val_loss: 0.5623\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 963us/step - loss: 0.5518 - val_loss: 0.5529\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 966us/step - loss: 0.5425 - val_loss: 0.5442\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 927us/step - loss: 0.5342 - val_loss: 0.5363\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 963us/step - loss: 0.5268 - val_loss: 0.5291\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5201 - val_loss: 0.5226\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 962us/step - loss: 0.5141 - val_loss: 0.5166\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 952us/step - loss: 0.5084 - val_loss: 0.5112\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 904us/step - loss: 0.5033 - val_loss: 0.5058\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 894us/step - loss: 0.4986 - val_loss: 0.5010\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 888us/step - loss: 0.4940 - val_loss: 0.4965\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 906us/step - loss: 0.4896 - val_loss: 0.4921\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 967us/step - loss: 0.4858 - val_loss: 0.4885\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 958us/step - loss: 0.4825 - val_loss: 0.4852\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 949us/step - loss: 0.4788 - val_loss: 0.4816\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 989us/step - loss: 0.4756 - val_loss: 0.4787\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 959us/step - loss: 0.4726 - val_loss: 0.4759\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 967us/step - loss: 0.4696 - val_loss: 0.4732\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 895us/step - loss: 0.4667 - val_loss: 0.4704\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4641 - val_loss: 0.4679\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 865us/step - loss: 0.4613 - val_loss: 0.4648\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 947us/step - loss: 0.4590 - val_loss: 0.4631\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4566 - val_loss: 0.4606\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4543 - val_loss: 0.4583\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 900us/step - loss: 0.4516 - val_loss: 0.4555\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 940us/step - loss: 0.4501 - val_loss: 0.4543\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 933us/step - loss: 0.4478 - val_loss: 0.4523\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 997us/step - loss: 0.4454 - val_loss: 0.4497\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 950us/step - loss: 0.4438 - val_loss: 0.4482\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 963us/step - loss: 0.4416 - val_loss: 0.4467\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4395 - val_loss: 0.4435\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.4389 - val_loss: 0.4424\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4356 - val_loss: 0.4398\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 949us/step - loss: 0.4345 - val_loss: 0.4388\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 934us/step - loss: 0.4325 - val_loss: 0.4365\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 935us/step - loss: 0.4302 - val_loss: 0.4343\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 909us/step - loss: 0.4275 - val_loss: 0.4325\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4261 - val_loss: 0.4305\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.4238 - val_loss: 0.4285\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4217 - val_loss: 0.4270\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 942us/step - loss: 0.4198 - val_loss: 0.4248\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 997us/step - loss: 0.4182 - val_loss: 0.4236\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 971us/step - loss: 0.4165 - val_loss: 0.4218\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4144 - val_loss: 0.4205\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 983us/step - loss: 0.4129 - val_loss: 0.4187\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 976us/step - loss: 0.4109 - val_loss: 0.4170\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 967us/step - loss: 0.4090 - val_loss: 0.4151\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 907us/step - loss: 0.4073 - val_loss: 0.4150\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 994us/step - loss: 0.4064 - val_loss: 0.4125\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 963us/step - loss: 0.4043 - val_loss: 0.4112\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 948us/step - loss: 0.4029 - val_loss: 0.4099\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4012 - val_loss: 0.4079\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3997 - val_loss: 0.4061\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 932us/step - loss: 0.3982 - val_loss: 0.4058\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3968 - val_loss: 0.4038\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 965us/step - loss: 0.3954 - val_loss: 0.4023\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.3942 - val_loss: 0.4007\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 952us/step - loss: 0.3925 - val_loss: 0.4009\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 982us/step - loss: 0.3910 - val_loss: 0.3986\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 995us/step - loss: 0.3898 - val_loss: 0.3969\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 997us/step - loss: 0.3884 - val_loss: 0.3960\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3873 - val_loss: 0.3959\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3858 - val_loss: 0.3933\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3848 - val_loss: 0.3928\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 925us/step - loss: 0.3836 - val_loss: 0.3912\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3823 - val_loss: 0.3900\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3810 - val_loss: 0.3903\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3802 - val_loss: 0.3881\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3790 - val_loss: 0.3870\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3780 - val_loss: 0.3863\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3769 - val_loss: 0.3850\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3755 - val_loss: 0.3841\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3749 - val_loss: 0.3835\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 957us/step - loss: 0.3737 - val_loss: 0.3820\n",
      "121/121 [==============================] - 0s 649us/step - loss: 0.3920\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\ML_PATH\\my_env\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 1s 1ms/step - loss: 4.2792 - val_loss: 3.1091\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 2.4585 - val_loss: 1.8605\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.6115 - val_loss: 1.3939\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3395 - val_loss: 1.2758\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.2684 - val_loss: 1.2368\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 939us/step - loss: 1.2308 - val_loss: 1.1986\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.1867 - val_loss: 1.1471\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.1279 - val_loss: 1.0802\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 997us/step - loss: 1.0547 - val_loss: 0.9994\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 942us/step - loss: 0.9696 - val_loss: 0.9087\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 948us/step - loss: 0.8770 - val_loss: 0.8154\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 948us/step - loss: 0.7879 - val_loss: 0.7321\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 920us/step - loss: 0.7125 - val_loss: 0.6692\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 926us/step - loss: 0.6578 - val_loss: 0.6279\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6226 - val_loss: 0.6030\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.5999 - val_loss: 0.5869\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5839 - val_loss: 0.5746\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 872us/step - loss: 0.5709 - val_loss: 0.5642\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 889us/step - loss: 0.5598 - val_loss: 0.5546\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5498 - val_loss: 0.5459\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.5408 - val_loss: 0.5378\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 887us/step - loss: 0.5323 - val_loss: 0.5301\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5249 - val_loss: 0.5233\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 886us/step - loss: 0.5179 - val_loss: 0.5169\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 937us/step - loss: 0.5113 - val_loss: 0.5107\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 899us/step - loss: 0.5055 - val_loss: 0.5068\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5004 - val_loss: 0.5008\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4956 - val_loss: 0.4968\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 910us/step - loss: 0.4916 - val_loss: 0.4936\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4875 - val_loss: 0.4898\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4840 - val_loss: 0.4865\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4805 - val_loss: 0.4829\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4779 - val_loss: 0.4804\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 910us/step - loss: 0.4751 - val_loss: 0.4779\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 910us/step - loss: 0.4728 - val_loss: 0.4753\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 932us/step - loss: 0.4701 - val_loss: 0.4727\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 958us/step - loss: 0.4679 - val_loss: 0.4716\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 926us/step - loss: 0.4657 - val_loss: 0.4704\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 950us/step - loss: 0.4634 - val_loss: 0.4670\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4619 - val_loss: 0.4655\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4599 - val_loss: 0.4635\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4579 - val_loss: 0.4629\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 893us/step - loss: 0.4562 - val_loss: 0.4595\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4544 - val_loss: 0.4580\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 900us/step - loss: 0.4525 - val_loss: 0.4561\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 949us/step - loss: 0.4513 - val_loss: 0.4554\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.4498 - val_loss: 0.4533\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 900us/step - loss: 0.4478 - val_loss: 0.4518\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 910us/step - loss: 0.4462 - val_loss: 0.4501\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 888us/step - loss: 0.4449 - val_loss: 0.4492\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 907us/step - loss: 0.4434 - val_loss: 0.4483\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 918us/step - loss: 0.4419 - val_loss: 0.4459\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 856us/step - loss: 0.4406 - val_loss: 0.4451\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 902us/step - loss: 0.4389 - val_loss: 0.4432\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.4380 - val_loss: 0.4421\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 847us/step - loss: 0.4363 - val_loss: 0.4410\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 940us/step - loss: 0.4348 - val_loss: 0.4390\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 866us/step - loss: 0.4337 - val_loss: 0.4386\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 908us/step - loss: 0.4320 - val_loss: 0.4368\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 900us/step - loss: 0.4314 - val_loss: 0.4359\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 899us/step - loss: 0.4300 - val_loss: 0.4349\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 907us/step - loss: 0.4282 - val_loss: 0.4329\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 885us/step - loss: 0.4275 - val_loss: 0.4315\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 963us/step - loss: 0.4262 - val_loss: 0.4304\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 928us/step - loss: 0.4251 - val_loss: 0.4296\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 902us/step - loss: 0.4238 - val_loss: 0.4284\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 897us/step - loss: 0.4227 - val_loss: 0.4274\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 882us/step - loss: 0.4215 - val_loss: 0.4258\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 933us/step - loss: 0.4199 - val_loss: 0.4260\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4192 - val_loss: 0.4254\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4181 - val_loss: 0.4234\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4169 - val_loss: 0.4216\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4160 - val_loss: 0.4213\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4147 - val_loss: 0.4200\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4135 - val_loss: 0.4182\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4127 - val_loss: 0.4182\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4113 - val_loss: 0.4173\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4104 - val_loss: 0.4158\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4092 - val_loss: 0.4180\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4085 - val_loss: 0.4141\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4072 - val_loss: 0.4125\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4063 - val_loss: 0.4133\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4050 - val_loss: 0.4120\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4044 - val_loss: 0.4094\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.4031 - val_loss: 0.4098\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4023 - val_loss: 0.4095\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4012 - val_loss: 0.4075\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 947us/step - loss: 0.4006 - val_loss: 0.4062\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3991 - val_loss: 0.4056\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3982 - val_loss: 0.4054\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3973 - val_loss: 0.4036\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3968 - val_loss: 0.4031\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3955 - val_loss: 0.4025\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3946 - val_loss: 0.4015\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 0.3937 - val_loss: 0.4003\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3925 - val_loss: 0.4019\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 902us/step - loss: 0.3920 - val_loss: 0.4006\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 938us/step - loss: 0.3911 - val_loss: 0.3976\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 889us/step - loss: 0.3902 - val_loss: 0.3969\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 890us/step - loss: 0.3895 - val_loss: 0.3958\n",
      "121/121 [==============================] - 0s 621us/step - loss: 0.3955\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\ML_PATH\\my_env\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 1s 2ms/step - loss: 4.1670 - val_loss: 3.1081\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 880us/step - loss: 2.4284 - val_loss: 1.9424\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 925us/step - loss: 1.6528 - val_loss: 1.4884\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3822 - val_loss: 1.3521\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.3056 - val_loss: 1.3145\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 949us/step - loss: 1.2818 - val_loss: 1.2991\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.2696 - val_loss: 1.2880\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.2585 - val_loss: 1.2759\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.2459 - val_loss: 1.2616\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 2ms/step - loss: 1.2305 - val_loss: 1.2441\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 914us/step - loss: 1.2094 - val_loss: 1.2128\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 855us/step - loss: 1.1662 - val_loss: 1.1592\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 923us/step - loss: 1.1101 - val_loss: 1.0978\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 937us/step - loss: 1.0468 - val_loss: 1.0306\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 0.9793 - val_loss: 0.9598\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 922us/step - loss: 0.9107 - val_loss: 0.8899\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 843us/step - loss: 0.8455 - val_loss: 0.8249\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 947us/step - loss: 0.7878 - val_loss: 0.7709\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 908us/step - loss: 0.7419 - val_loss: 0.7293\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 848us/step - loss: 0.7087 - val_loss: 0.6985\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.6839 - val_loss: 0.6759\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 934us/step - loss: 0.6650 - val_loss: 0.6587\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 954us/step - loss: 0.6498 - val_loss: 0.6446\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 922us/step - loss: 0.6372 - val_loss: 0.6326\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 917us/step - loss: 0.6260 - val_loss: 0.6218\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 912us/step - loss: 0.6153 - val_loss: 0.6117\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 898us/step - loss: 0.6053 - val_loss: 0.6019\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 887us/step - loss: 0.5957 - val_loss: 0.5927\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 899us/step - loss: 0.5864 - val_loss: 0.5836\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5771 - val_loss: 0.5747\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 863us/step - loss: 0.5680 - val_loss: 0.5663\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 953us/step - loss: 0.5590 - val_loss: 0.5577\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 990us/step - loss: 0.5500 - val_loss: 0.5492\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 938us/step - loss: 0.5414 - val_loss: 0.5405\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 885us/step - loss: 0.5327 - val_loss: 0.5326\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 891us/step - loss: 0.5243 - val_loss: 0.5246\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 869us/step - loss: 0.5162 - val_loss: 0.5170\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 912us/step - loss: 0.5085 - val_loss: 0.5095\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 879us/step - loss: 0.5012 - val_loss: 0.5021\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4940 - val_loss: 0.4953\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 946us/step - loss: 0.4872 - val_loss: 0.4885\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4807 - val_loss: 0.4829\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 859us/step - loss: 0.4746 - val_loss: 0.4765\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 919us/step - loss: 0.4686 - val_loss: 0.4716\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 920us/step - loss: 0.4633 - val_loss: 0.4657\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 919us/step - loss: 0.4581 - val_loss: 0.4612\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4533 - val_loss: 0.4573\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 910us/step - loss: 0.4490 - val_loss: 0.4524\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 886us/step - loss: 0.4448 - val_loss: 0.4496\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 856us/step - loss: 0.4411 - val_loss: 0.4459\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 902us/step - loss: 0.4380 - val_loss: 0.4429\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 935us/step - loss: 0.4349 - val_loss: 0.4397\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.4318 - val_loss: 0.4368\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 901us/step - loss: 0.4290 - val_loss: 0.4340\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 867us/step - loss: 0.4267 - val_loss: 0.4316\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 956us/step - loss: 0.4243 - val_loss: 0.4299\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 916us/step - loss: 0.4221 - val_loss: 0.4282\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.4200 - val_loss: 0.4257\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4180 - val_loss: 0.4235\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4160 - val_loss: 0.4226\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4141 - val_loss: 0.4201\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4126 - val_loss: 0.4192\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 852us/step - loss: 0.4109 - val_loss: 0.4169\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 858us/step - loss: 0.4093 - val_loss: 0.4159\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 873us/step - loss: 0.4075 - val_loss: 0.4140\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 871us/step - loss: 0.4058 - val_loss: 0.4127\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.4048 - val_loss: 0.4119\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4032 - val_loss: 0.4105\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4020 - val_loss: 0.4098\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 882us/step - loss: 0.4004 - val_loss: 0.4076\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 871us/step - loss: 0.3994 - val_loss: 0.4063\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 959us/step - loss: 0.3979 - val_loss: 0.4059\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 916us/step - loss: 0.3966 - val_loss: 0.4041\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 922us/step - loss: 0.3956 - val_loss: 0.4028\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 904us/step - loss: 0.3943 - val_loss: 0.4019\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 926us/step - loss: 0.3934 - val_loss: 0.4008\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 910us/step - loss: 0.3921 - val_loss: 0.4000\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 935us/step - loss: 0.3909 - val_loss: 0.3997\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 915us/step - loss: 0.3899 - val_loss: 0.3978\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 891us/step - loss: 0.3890 - val_loss: 0.3983\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 890us/step - loss: 0.3880 - val_loss: 0.3979\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 890us/step - loss: 0.3871 - val_loss: 0.3955\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 894us/step - loss: 0.3860 - val_loss: 0.3945\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3853 - val_loss: 0.3938\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3843 - val_loss: 0.3932\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 948us/step - loss: 0.3833 - val_loss: 0.3920\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3826 - val_loss: 0.3924\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3816 - val_loss: 0.3927\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3809 - val_loss: 0.3919\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 894us/step - loss: 0.3800 - val_loss: 0.3889\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3791 - val_loss: 0.3895\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.3782 - val_loss: 0.3885\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 867us/step - loss: 0.3771 - val_loss: 0.3873\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 920us/step - loss: 0.3771 - val_loss: 0.3862\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 908us/step - loss: 0.3756 - val_loss: 0.3868\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 878us/step - loss: 0.3752 - val_loss: 0.3846\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 922us/step - loss: 0.3747 - val_loss: 0.3843\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 861us/step - loss: 0.3738 - val_loss: 0.3831\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 908us/step - loss: 0.3730 - val_loss: 0.3833\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 894us/step - loss: 0.3723 - val_loss: 0.3843\n",
      "121/121 [==============================] - 0s 598us/step - loss: 0.3729\n",
      "Epoch 1/100\n",
      "  1/242 [..............................] - ETA: 33s - loss: 6.2672"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\ML_PATH\\my_env\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 1ms/step - loss: 5.4342 - val_loss: 4.0316\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 819us/step - loss: 3.3588 - val_loss: 2.7248\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 824us/step - loss: 2.3798 - val_loss: 2.0075\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 841us/step - loss: 1.8082 - val_loss: 1.5767\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 806us/step - loss: 1.4534 - val_loss: 1.3017\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.2148 - val_loss: 1.1107\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 1.0422 - val_loss: 0.9696\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 743us/step - loss: 0.9114 - val_loss: 0.8622\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 811us/step - loss: 0.8146 - val_loss: 0.7828\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 892us/step - loss: 0.7451 - val_loss: 0.7256\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 862us/step - loss: 0.6970 - val_loss: 0.6867\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 870us/step - loss: 0.6646 - val_loss: 0.6604\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 822us/step - loss: 0.6430 - val_loss: 0.6430\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 811us/step - loss: 0.6284 - val_loss: 0.6305\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 839us/step - loss: 0.6181 - val_loss: 0.6212\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 876us/step - loss: 0.6103 - val_loss: 0.6139\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 825us/step - loss: 0.6039 - val_loss: 0.6079\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 845us/step - loss: 0.5983 - val_loss: 0.6025\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 848us/step - loss: 0.5934 - val_loss: 0.5977\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 787us/step - loss: 0.5888 - val_loss: 0.5933\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 867us/step - loss: 0.5847 - val_loss: 0.5890\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 868us/step - loss: 0.5806 - val_loss: 0.5848\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 803us/step - loss: 0.5765 - val_loss: 0.5806\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 842us/step - loss: 0.5727 - val_loss: 0.5766\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 844us/step - loss: 0.5689 - val_loss: 0.5727\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 796us/step - loss: 0.5652 - val_loss: 0.5689\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 762us/step - loss: 0.5616 - val_loss: 0.5651\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 832us/step - loss: 0.5580 - val_loss: 0.5614\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 785us/step - loss: 0.5543 - val_loss: 0.5575\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 823us/step - loss: 0.5505 - val_loss: 0.5538\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 850us/step - loss: 0.5469 - val_loss: 0.5502\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 805us/step - loss: 0.5433 - val_loss: 0.5467\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 790us/step - loss: 0.5398 - val_loss: 0.5433\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 806us/step - loss: 0.5364 - val_loss: 0.5401\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 848us/step - loss: 0.5332 - val_loss: 0.5369\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 832us/step - loss: 0.5300 - val_loss: 0.5340\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 853us/step - loss: 0.5270 - val_loss: 0.5310\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 863us/step - loss: 0.5241 - val_loss: 0.5281\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 812us/step - loss: 0.5213 - val_loss: 0.5254\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 818us/step - loss: 0.5186 - val_loss: 0.5228\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 798us/step - loss: 0.5160 - val_loss: 0.5201\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 837us/step - loss: 0.5136 - val_loss: 0.5178\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 811us/step - loss: 0.5112 - val_loss: 0.5153\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 816us/step - loss: 0.5089 - val_loss: 0.5131\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 812us/step - loss: 0.5067 - val_loss: 0.5108\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 831us/step - loss: 0.5046 - val_loss: 0.5087\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 853us/step - loss: 0.5025 - val_loss: 0.5065\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 824us/step - loss: 0.5005 - val_loss: 0.5044\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 799us/step - loss: 0.4987 - val_loss: 0.5026\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 817us/step - loss: 0.4968 - val_loss: 0.5009\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 834us/step - loss: 0.4949 - val_loss: 0.4989\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 810us/step - loss: 0.4931 - val_loss: 0.4970\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 811us/step - loss: 0.4916 - val_loss: 0.4956\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 766us/step - loss: 0.4899 - val_loss: 0.4939\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 864us/step - loss: 0.4883 - val_loss: 0.4922\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 871us/step - loss: 0.4869 - val_loss: 0.4910\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 815us/step - loss: 0.4853 - val_loss: 0.4891\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 868us/step - loss: 0.4841 - val_loss: 0.4879\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 845us/step - loss: 0.4826 - val_loss: 0.4863\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 805us/step - loss: 0.4814 - val_loss: 0.4849\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 802us/step - loss: 0.4802 - val_loss: 0.4839\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 868us/step - loss: 0.4789 - val_loss: 0.4830\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 806us/step - loss: 0.4777 - val_loss: 0.4819\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 870us/step - loss: 0.4766 - val_loss: 0.4806\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 773us/step - loss: 0.4754 - val_loss: 0.4793\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 810us/step - loss: 0.4741 - val_loss: 0.4777\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 808us/step - loss: 0.4734 - val_loss: 0.4769\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 840us/step - loss: 0.4721 - val_loss: 0.4756\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 796us/step - loss: 0.4712 - val_loss: 0.4751\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.4703 - val_loss: 0.4743\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 828us/step - loss: 0.4696 - val_loss: 0.4732\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 859us/step - loss: 0.4686 - val_loss: 0.4724\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 844us/step - loss: 0.4674 - val_loss: 0.4707\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 831us/step - loss: 0.4671 - val_loss: 0.4704\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 785us/step - loss: 0.4660 - val_loss: 0.4699\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 830us/step - loss: 0.4653 - val_loss: 0.4691\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 816us/step - loss: 0.4643 - val_loss: 0.4675\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 751us/step - loss: 0.4636 - val_loss: 0.4672\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 814us/step - loss: 0.4627 - val_loss: 0.4658\n",
      "Epoch 80/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 868us/step - loss: 0.4621 - val_loss: 0.4649\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 837us/step - loss: 0.4617 - val_loss: 0.4650\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 849us/step - loss: 0.4604 - val_loss: 0.4635\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 802us/step - loss: 0.4602 - val_loss: 0.4628\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 817us/step - loss: 0.4596 - val_loss: 0.4627\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 858us/step - loss: 0.4587 - val_loss: 0.4617\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 808us/step - loss: 0.4579 - val_loss: 0.4607\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 849us/step - loss: 0.4576 - val_loss: 0.4600\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 816us/step - loss: 0.4571 - val_loss: 0.4598\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 813us/step - loss: 0.4561 - val_loss: 0.4588\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 854us/step - loss: 0.4559 - val_loss: 0.4584\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 871us/step - loss: 0.4552 - val_loss: 0.4590\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 829us/step - loss: 0.4548 - val_loss: 0.4581\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 818us/step - loss: 0.4542 - val_loss: 0.4574\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 874us/step - loss: 0.4535 - val_loss: 0.4564\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 794us/step - loss: 0.4530 - val_loss: 0.4565\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 813us/step - loss: 0.4526 - val_loss: 0.4557\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 845us/step - loss: 0.4519 - val_loss: 0.4556\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 791us/step - loss: 0.4511 - val_loss: 0.4539\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 791us/step - loss: 0.4513 - val_loss: 0.4538\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 834us/step - loss: 0.4505 - val_loss: 0.4531\n",
      "121/121 [==============================] - 0s 511us/step - loss: 0.4599\n",
      "Epoch 1/100\n",
      "  1/242 [..............................] - ETA: 34s - loss: 7.8790"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\ML_PATH\\my_env\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 1ms/step - loss: 5.5742 - val_loss: 4.2330\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 838us/step - loss: 3.6061 - val_loss: 3.0010\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 824us/step - loss: 2.7156 - val_loss: 2.3951\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 913us/step - loss: 2.2530 - val_loss: 2.0582\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 771us/step - loss: 1.9782 - val_loss: 1.8414\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 839us/step - loss: 1.7905 - val_loss: 1.6829\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 868us/step - loss: 1.6448 - val_loss: 1.5521\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 864us/step - loss: 1.5191 - val_loss: 1.4381\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 864us/step - loss: 1.4039 - val_loss: 1.3327\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 884us/step - loss: 1.3011 - val_loss: 1.2402\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 830us/step - loss: 1.2106 - val_loss: 1.1584\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 785us/step - loss: 1.1323 - val_loss: 1.0878\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 826us/step - loss: 1.0662 - val_loss: 1.0286\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 837us/step - loss: 1.0102 - val_loss: 0.9793\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 820us/step - loss: 0.9645 - val_loss: 0.9384\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 786us/step - loss: 0.9269 - val_loss: 0.9047\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 824us/step - loss: 0.8961 - val_loss: 0.8777\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 780us/step - loss: 0.8707 - val_loss: 0.8553\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 798us/step - loss: 0.8491 - val_loss: 0.8365\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 777us/step - loss: 0.8313 - val_loss: 0.8209\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 889us/step - loss: 0.8163 - val_loss: 0.8079\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 863us/step - loss: 0.8030 - val_loss: 0.7968\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 800us/step - loss: 0.7910 - val_loss: 0.7864\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 885us/step - loss: 0.7809 - val_loss: 0.7775\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 904us/step - loss: 0.7711 - val_loss: 0.7690\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 866us/step - loss: 0.7626 - val_loss: 0.7613\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 785us/step - loss: 0.7544 - val_loss: 0.7541\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 806us/step - loss: 0.7465 - val_loss: 0.7474\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 846us/step - loss: 0.7390 - val_loss: 0.7408\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 983us/step - loss: 0.7321 - val_loss: 0.7345\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 967us/step - loss: 0.7256 - val_loss: 0.7285\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 986us/step - loss: 0.7192 - val_loss: 0.7228\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7131 - val_loss: 0.7173\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 976us/step - loss: 0.7073 - val_loss: 0.7119\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.7018 - val_loss: 0.7069\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6965 - val_loss: 0.7019\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.6914 - val_loss: 0.6972\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 881us/step - loss: 0.6864 - val_loss: 0.6925\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 831us/step - loss: 0.6816 - val_loss: 0.6880\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 880us/step - loss: 0.6770 - val_loss: 0.6836\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 807us/step - loss: 0.6725 - val_loss: 0.6793\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 873us/step - loss: 0.6682 - val_loss: 0.6750\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 836us/step - loss: 0.6640 - val_loss: 0.6709\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 881us/step - loss: 0.6598 - val_loss: 0.6669\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 806us/step - loss: 0.6560 - val_loss: 0.6629\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 860us/step - loss: 0.6522 - val_loss: 0.6589\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 818us/step - loss: 0.6485 - val_loss: 0.6550\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 911us/step - loss: 0.6448 - val_loss: 0.6513\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 839us/step - loss: 0.6413 - val_loss: 0.6476\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 828us/step - loss: 0.6377 - val_loss: 0.6440\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 861us/step - loss: 0.6343 - val_loss: 0.6405\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 801us/step - loss: 0.6309 - val_loss: 0.6370\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 809us/step - loss: 0.6276 - val_loss: 0.6336\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 842us/step - loss: 0.6243 - val_loss: 0.6301\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 896us/step - loss: 0.6210 - val_loss: 0.6268\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 800us/step - loss: 0.6178 - val_loss: 0.6235\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 831us/step - loss: 0.6146 - val_loss: 0.6203\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 784us/step - loss: 0.6115 - val_loss: 0.6171\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 0.6085 - val_loss: 0.6140\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 819us/step - loss: 0.6055 - val_loss: 0.6109\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 883us/step - loss: 0.6026 - val_loss: 0.6078\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 789us/step - loss: 0.5996 - val_loss: 0.6047\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 839us/step - loss: 0.5968 - val_loss: 0.6018\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 874us/step - loss: 0.5940 - val_loss: 0.5990\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 890us/step - loss: 0.5913 - val_loss: 0.5961\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 798us/step - loss: 0.5887 - val_loss: 0.5934\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 919us/step - loss: 0.5861 - val_loss: 0.5907\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 885us/step - loss: 0.5836 - val_loss: 0.5881\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 854us/step - loss: 0.5811 - val_loss: 0.5856\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 868us/step - loss: 0.5787 - val_loss: 0.5832\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 817us/step - loss: 0.5764 - val_loss: 0.5807\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 794us/step - loss: 0.5741 - val_loss: 0.5781\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 821us/step - loss: 0.5719 - val_loss: 0.5759\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 836us/step - loss: 0.5698 - val_loss: 0.5737\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 872us/step - loss: 0.5676 - val_loss: 0.5714\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 818us/step - loss: 0.5656 - val_loss: 0.5694\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 847us/step - loss: 0.5636 - val_loss: 0.5671\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 803us/step - loss: 0.5616 - val_loss: 0.5649\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 842us/step - loss: 0.5597 - val_loss: 0.5629\n",
      "Epoch 80/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 898us/step - loss: 0.5577 - val_loss: 0.5609\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 853us/step - loss: 0.5558 - val_loss: 0.5588\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 809us/step - loss: 0.5539 - val_loss: 0.5568\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 816us/step - loss: 0.5521 - val_loss: 0.5548\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 889us/step - loss: 0.5503 - val_loss: 0.5533\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 857us/step - loss: 0.5485 - val_loss: 0.5513\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 868us/step - loss: 0.5468 - val_loss: 0.5497\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 809us/step - loss: 0.5452 - val_loss: 0.5478\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 771us/step - loss: 0.5436 - val_loss: 0.5463\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 819us/step - loss: 0.5420 - val_loss: 0.5448\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 859us/step - loss: 0.5405 - val_loss: 0.5432\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 822us/step - loss: 0.5390 - val_loss: 0.5413\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 861us/step - loss: 0.5376 - val_loss: 0.5397\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 875us/step - loss: 0.5361 - val_loss: 0.5381\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 838us/step - loss: 0.5348 - val_loss: 0.5368\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 858us/step - loss: 0.5334 - val_loss: 0.5356\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 780us/step - loss: 0.5320 - val_loss: 0.5340\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 829us/step - loss: 0.5308 - val_loss: 0.5326\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 827us/step - loss: 0.5296 - val_loss: 0.5319\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 913us/step - loss: 0.5282 - val_loss: 0.5303\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 795us/step - loss: 0.5271 - val_loss: 0.5296\n",
      "121/121 [==============================] - 0s 529us/step - loss: 0.5278\n",
      "Epoch 1/100\n",
      "  1/242 [..............................] - ETA: 34s - loss: 10.1125"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\ML_PATH\\my_env\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\gradient_descent.py:108: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 1ms/step - loss: 6.9551 - val_loss: 4.8369\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 782us/step - loss: 3.8740 - val_loss: 3.0694\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 851us/step - loss: 2.6126 - val_loss: 2.2246\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 856us/step - loss: 1.9767 - val_loss: 1.7730\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 879us/step - loss: 1.6281 - val_loss: 1.5178\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 905us/step - loss: 1.4254 - val_loss: 1.3629\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 831us/step - loss: 1.2991 - val_loss: 1.2622\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 838us/step - loss: 1.2143 - val_loss: 1.1909\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 838us/step - loss: 1.1525 - val_loss: 1.1367\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 859us/step - loss: 1.1042 - val_loss: 1.0929\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 843us/step - loss: 1.0643 - val_loss: 1.0558\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 883us/step - loss: 1.0303 - val_loss: 1.0234\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 840us/step - loss: 1.0006 - val_loss: 0.9947\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 804us/step - loss: 0.9742 - val_loss: 0.9688\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 863us/step - loss: 0.9505 - val_loss: 0.9456\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 801us/step - loss: 0.9289 - val_loss: 0.9243\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 826us/step - loss: 0.9090 - val_loss: 0.9044\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 802us/step - loss: 0.8905 - val_loss: 0.8860\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 827us/step - loss: 0.8735 - val_loss: 0.8689\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 825us/step - loss: 0.8577 - val_loss: 0.8530\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 916us/step - loss: 0.8429 - val_loss: 0.8382\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 872us/step - loss: 0.8289 - val_loss: 0.8243\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 805us/step - loss: 0.8158 - val_loss: 0.8113\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 764us/step - loss: 0.8034 - val_loss: 0.7991\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 921us/step - loss: 0.7916 - val_loss: 0.7876\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 894us/step - loss: 0.7803 - val_loss: 0.7766\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 868us/step - loss: 0.7696 - val_loss: 0.7664\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 772us/step - loss: 0.7595 - val_loss: 0.7569\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 835us/step - loss: 0.7499 - val_loss: 0.7478\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 784us/step - loss: 0.7406 - val_loss: 0.7390\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 830us/step - loss: 0.7317 - val_loss: 0.7305\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 879us/step - loss: 0.7231 - val_loss: 0.7224\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 813us/step - loss: 0.7148 - val_loss: 0.7146\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 852us/step - loss: 0.7069 - val_loss: 0.7072\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 878us/step - loss: 0.6993 - val_loss: 0.7000\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 769us/step - loss: 0.6920 - val_loss: 0.6930\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 813us/step - loss: 0.6848 - val_loss: 0.6861\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 878us/step - loss: 0.6779 - val_loss: 0.6794\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 788us/step - loss: 0.6713 - val_loss: 0.6729\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 829us/step - loss: 0.6648 - val_loss: 0.6665\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 807us/step - loss: 0.6587 - val_loss: 0.6603\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 833us/step - loss: 0.6528 - val_loss: 0.6544\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 880us/step - loss: 0.6471 - val_loss: 0.6487\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 819us/step - loss: 0.6416 - val_loss: 0.6433\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 813us/step - loss: 0.6364 - val_loss: 0.6380\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 812us/step - loss: 0.6314 - val_loss: 0.6330\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 835us/step - loss: 0.6267 - val_loss: 0.6282\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 891us/step - loss: 0.6223 - val_loss: 0.6237\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 849us/step - loss: 0.6180 - val_loss: 0.6192\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 813us/step - loss: 0.6139 - val_loss: 0.6150\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 881us/step - loss: 0.6100 - val_loss: 0.6110\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 772us/step - loss: 0.6064 - val_loss: 0.6072\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 856us/step - loss: 0.6028 - val_loss: 0.6035\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 812us/step - loss: 0.5994 - val_loss: 0.6000\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 880us/step - loss: 0.5962 - val_loss: 0.5967\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 922us/step - loss: 0.5932 - val_loss: 0.5936\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 832us/step - loss: 0.5903 - val_loss: 0.5906\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 877us/step - loss: 0.5876 - val_loss: 0.5877\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 864us/step - loss: 0.5850 - val_loss: 0.5850\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 856us/step - loss: 0.5824 - val_loss: 0.5824\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 874us/step - loss: 0.5800 - val_loss: 0.5799\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 875us/step - loss: 0.5777 - val_loss: 0.5776\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 861us/step - loss: 0.5755 - val_loss: 0.5754\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 860us/step - loss: 0.5734 - val_loss: 0.5733\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 862us/step - loss: 0.5714 - val_loss: 0.5713\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 788us/step - loss: 0.5695 - val_loss: 0.5694\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 850us/step - loss: 0.5676 - val_loss: 0.5676\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 879us/step - loss: 0.5659 - val_loss: 0.5660\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 973us/step - loss: 0.5642 - val_loss: 0.5643\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5626 - val_loss: 0.5628\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5610 - val_loss: 0.5613\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5594 - val_loss: 0.5598\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 858us/step - loss: 0.5580 - val_loss: 0.5585\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.5565 - val_loss: 0.5572\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 864us/step - loss: 0.5551 - val_loss: 0.5559\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 887us/step - loss: 0.5539 - val_loss: 0.5546\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 849us/step - loss: 0.5526 - val_loss: 0.5534\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 869us/step - loss: 0.5514 - val_loss: 0.5522\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 837us/step - loss: 0.5502 - val_loss: 0.5510\n",
      "Epoch 80/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 840us/step - loss: 0.5491 - val_loss: 0.5499\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 931us/step - loss: 0.5479 - val_loss: 0.5488\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 820us/step - loss: 0.5468 - val_loss: 0.5477\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 894us/step - loss: 0.5457 - val_loss: 0.5467\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 798us/step - loss: 0.5447 - val_loss: 0.5458\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 878us/step - loss: 0.5437 - val_loss: 0.5448\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 839us/step - loss: 0.5427 - val_loss: 0.5439\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 876us/step - loss: 0.5418 - val_loss: 0.5430\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 902us/step - loss: 0.5408 - val_loss: 0.5421\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 882us/step - loss: 0.5399 - val_loss: 0.5412\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 831us/step - loss: 0.5389 - val_loss: 0.5403\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 874us/step - loss: 0.5380 - val_loss: 0.5395\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 956us/step - loss: 0.5371 - val_loss: 0.5387\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 831us/step - loss: 0.5362 - val_loss: 0.5379\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 819us/step - loss: 0.5353 - val_loss: 0.5371\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 814us/step - loss: 0.5345 - val_loss: 0.5364\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 992us/step - loss: 0.5337 - val_loss: 0.5357\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 873us/step - loss: 0.5329 - val_loss: 0.5349\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 869us/step - loss: 0.5322 - val_loss: 0.5342\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 841us/step - loss: 0.5314 - val_loss: 0.5335\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 871us/step - loss: 0.5307 - val_loss: 0.5329\n",
      "121/121 [==============================] - 0s 642us/step - loss: 0.5249\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\ML_PATH\\my_env\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "6 fits failed out of a total of 30.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "6 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Admin\\ML_PATH\\my_env\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Admin\\ML_PATH\\my_env\\lib\\site-packages\\keras\\wrappers\\scikit_learn.py\", line 157, in fit\n",
      "    if (losses.is_categorical_crossentropy(self.model.loss) and\n",
      "AttributeError: 'Sequential' object has no attribute 'loss'\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\Admin\\ML_PATH\\my_env\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [        nan -0.32937615 -0.36942198 -0.37584699 -0.34110872         nan\n",
      "         nan -0.3519172  -0.3868174  -0.50418733]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "363/363 [==============================] - 0s 925us/step - loss: 1.3116 - val_loss: 0.6537\n",
      "Epoch 2/100\n",
      "363/363 [==============================] - 0s 766us/step - loss: 0.5995 - val_loss: 0.5741\n",
      "Epoch 3/100\n",
      "363/363 [==============================] - 0s 774us/step - loss: 0.5340 - val_loss: 0.5159\n",
      "Epoch 4/100\n",
      "363/363 [==============================] - 0s 759us/step - loss: 0.4822 - val_loss: 0.4707\n",
      "Epoch 5/100\n",
      "363/363 [==============================] - 0s 808us/step - loss: 0.4452 - val_loss: 0.4401\n",
      "Epoch 6/100\n",
      "363/363 [==============================] - 0s 784us/step - loss: 0.4232 - val_loss: 0.4242\n",
      "Epoch 7/100\n",
      "363/363 [==============================] - 0s 741us/step - loss: 0.4100 - val_loss: 0.4138\n",
      "Epoch 8/100\n",
      "363/363 [==============================] - 0s 793us/step - loss: 0.3998 - val_loss: 0.4041\n",
      "Epoch 9/100\n",
      "363/363 [==============================] - 0s 796us/step - loss: 0.3926 - val_loss: 0.4031\n",
      "Epoch 10/100\n",
      "363/363 [==============================] - 0s 737us/step - loss: 0.3864 - val_loss: 0.3896\n",
      "Epoch 11/100\n",
      "363/363 [==============================] - 0s 760us/step - loss: 0.3820 - val_loss: 0.3860\n",
      "Epoch 12/100\n",
      "363/363 [==============================] - 0s 805us/step - loss: 0.3780 - val_loss: 0.3847\n",
      "Epoch 13/100\n",
      "363/363 [==============================] - 0s 774us/step - loss: 0.3740 - val_loss: 0.3785\n",
      "Epoch 14/100\n",
      "363/363 [==============================] - 0s 804us/step - loss: 0.3712 - val_loss: 0.3748\n",
      "Epoch 15/100\n",
      "363/363 [==============================] - 0s 761us/step - loss: 0.3679 - val_loss: 0.3736\n",
      "Epoch 16/100\n",
      "363/363 [==============================] - 0s 794us/step - loss: 0.3657 - val_loss: 0.3743\n",
      "Epoch 17/100\n",
      "363/363 [==============================] - 0s 741us/step - loss: 0.3629 - val_loss: 0.3753\n",
      "Epoch 18/100\n",
      "363/363 [==============================] - 0s 757us/step - loss: 0.3619 - val_loss: 0.3686\n",
      "Epoch 19/100\n",
      "363/363 [==============================] - 0s 781us/step - loss: 0.3597 - val_loss: 0.3654\n",
      "Epoch 20/100\n",
      "363/363 [==============================] - 0s 749us/step - loss: 0.3564 - val_loss: 0.3674\n",
      "Epoch 21/100\n",
      "363/363 [==============================] - 0s 811us/step - loss: 0.3554 - val_loss: 0.3699\n",
      "Epoch 22/100\n",
      "363/363 [==============================] - 0s 803us/step - loss: 0.3545 - val_loss: 0.3634\n",
      "Epoch 23/100\n",
      "363/363 [==============================] - 0s 792us/step - loss: 0.3527 - val_loss: 0.3638\n",
      "Epoch 24/100\n",
      "363/363 [==============================] - 0s 792us/step - loss: 0.3510 - val_loss: 0.3600\n",
      "Epoch 25/100\n",
      "363/363 [==============================] - 0s 791us/step - loss: 0.3506 - val_loss: 0.3589\n",
      "Epoch 26/100\n",
      "363/363 [==============================] - 0s 774us/step - loss: 0.3493 - val_loss: 0.3600\n",
      "Epoch 27/100\n",
      "363/363 [==============================] - 0s 745us/step - loss: 0.3477 - val_loss: 0.3585\n",
      "Epoch 28/100\n",
      "363/363 [==============================] - 0s 782us/step - loss: 0.3470 - val_loss: 0.3599\n",
      "Epoch 29/100\n",
      "363/363 [==============================] - 0s 759us/step - loss: 0.3458 - val_loss: 0.3599\n",
      "Epoch 30/100\n",
      "363/363 [==============================] - 0s 756us/step - loss: 0.3444 - val_loss: 0.3559\n",
      "Epoch 31/100\n",
      "363/363 [==============================] - 0s 763us/step - loss: 0.3437 - val_loss: 0.3611\n",
      "Epoch 32/100\n",
      "363/363 [==============================] - 0s 781us/step - loss: 0.3427 - val_loss: 0.3586\n",
      "Epoch 33/100\n",
      "363/363 [==============================] - 0s 800us/step - loss: 0.3416 - val_loss: 0.3538\n",
      "Epoch 34/100\n",
      "363/363 [==============================] - 0s 767us/step - loss: 0.3415 - val_loss: 0.3589\n",
      "Epoch 35/100\n",
      "363/363 [==============================] - 0s 758us/step - loss: 0.3407 - val_loss: 0.3533\n",
      "Epoch 36/100\n",
      "363/363 [==============================] - 0s 743us/step - loss: 0.3398 - val_loss: 0.3603\n",
      "Epoch 37/100\n",
      "363/363 [==============================] - 0s 757us/step - loss: 0.3387 - val_loss: 0.3544\n",
      "Epoch 38/100\n",
      "363/363 [==============================] - 0s 769us/step - loss: 0.3380 - val_loss: 0.3495\n",
      "Epoch 39/100\n",
      "363/363 [==============================] - 0s 777us/step - loss: 0.3375 - val_loss: 0.3498\n",
      "Epoch 40/100\n",
      "363/363 [==============================] - 0s 743us/step - loss: 0.3371 - val_loss: 0.3499\n",
      "Epoch 41/100\n",
      "363/363 [==============================] - 0s 808us/step - loss: 0.3353 - val_loss: 0.3522\n",
      "Epoch 42/100\n",
      "363/363 [==============================] - 0s 759us/step - loss: 0.3355 - val_loss: 0.3510\n",
      "Epoch 43/100\n",
      "363/363 [==============================] - 0s 784us/step - loss: 0.3350 - val_loss: 0.3538\n",
      "Epoch 44/100\n",
      "363/363 [==============================] - 0s 768us/step - loss: 0.3329 - val_loss: 0.3465\n",
      "Epoch 45/100\n",
      "363/363 [==============================] - 0s 767us/step - loss: 0.3334 - val_loss: 0.3441\n",
      "Epoch 46/100\n",
      "363/363 [==============================] - 0s 752us/step - loss: 0.3331 - val_loss: 0.3411\n",
      "Epoch 47/100\n",
      "363/363 [==============================] - 0s 791us/step - loss: 0.3312 - val_loss: 0.3424\n",
      "Epoch 48/100\n",
      "363/363 [==============================] - 0s 760us/step - loss: 0.3303 - val_loss: 0.3423\n",
      "Epoch 49/100\n",
      "363/363 [==============================] - 0s 717us/step - loss: 0.3296 - val_loss: 0.3518\n",
      "Epoch 50/100\n",
      "363/363 [==============================] - 0s 773us/step - loss: 0.3282 - val_loss: 0.3399\n",
      "Epoch 51/100\n",
      "363/363 [==============================] - 0s 763us/step - loss: 0.3271 - val_loss: 0.3365\n",
      "Epoch 52/100\n",
      "363/363 [==============================] - 0s 718us/step - loss: 0.3267 - val_loss: 0.3354\n",
      "Epoch 53/100\n",
      "363/363 [==============================] - 0s 753us/step - loss: 0.3244 - val_loss: 0.3395\n",
      "Epoch 54/100\n",
      "363/363 [==============================] - 0s 754us/step - loss: 0.3233 - val_loss: 0.3370\n",
      "Epoch 55/100\n",
      "363/363 [==============================] - 0s 739us/step - loss: 0.3235 - val_loss: 0.3362\n",
      "Epoch 56/100\n",
      "363/363 [==============================] - 0s 756us/step - loss: 0.3222 - val_loss: 0.3338\n",
      "Epoch 57/100\n",
      "363/363 [==============================] - 0s 786us/step - loss: 0.3213 - val_loss: 0.3387\n",
      "Epoch 58/100\n",
      "363/363 [==============================] - 0s 788us/step - loss: 0.3212 - val_loss: 0.3354\n",
      "Epoch 59/100\n",
      "363/363 [==============================] - 0s 767us/step - loss: 0.3201 - val_loss: 0.3382\n",
      "Epoch 60/100\n",
      "363/363 [==============================] - 0s 801us/step - loss: 0.3192 - val_loss: 0.3323\n",
      "Epoch 61/100\n",
      "363/363 [==============================] - 0s 705us/step - loss: 0.3202 - val_loss: 0.3305\n",
      "Epoch 62/100\n",
      "363/363 [==============================] - 0s 751us/step - loss: 0.3198 - val_loss: 0.3324\n",
      "Epoch 63/100\n",
      "363/363 [==============================] - 0s 742us/step - loss: 0.3190 - val_loss: 0.3380\n",
      "Epoch 64/100\n",
      "363/363 [==============================] - 0s 739us/step - loss: 0.3177 - val_loss: 0.3278\n",
      "Epoch 65/100\n",
      "363/363 [==============================] - 0s 795us/step - loss: 0.3154 - val_loss: 0.3289\n",
      "Epoch 66/100\n",
      "363/363 [==============================] - 0s 824us/step - loss: 0.3148 - val_loss: 0.3266\n",
      "Epoch 67/100\n",
      "363/363 [==============================] - 0s 708us/step - loss: 0.3145 - val_loss: 0.3361\n",
      "Epoch 68/100\n",
      "363/363 [==============================] - 0s 749us/step - loss: 0.3172 - val_loss: 0.3319\n",
      "Epoch 69/100\n",
      "363/363 [==============================] - 0s 729us/step - loss: 0.3145 - val_loss: 0.3286\n",
      "Epoch 70/100\n",
      "363/363 [==============================] - 0s 797us/step - loss: 0.3139 - val_loss: 0.3285\n",
      "Epoch 71/100\n",
      "363/363 [==============================] - 0s 763us/step - loss: 0.3135 - val_loss: 0.3303\n",
      "Epoch 72/100\n",
      "363/363 [==============================] - 0s 789us/step - loss: 0.3116 - val_loss: 0.3260\n",
      "Epoch 73/100\n",
      "363/363 [==============================] - 0s 767us/step - loss: 0.3126 - val_loss: 0.3239\n",
      "Epoch 74/100\n",
      "363/363 [==============================] - 0s 786us/step - loss: 0.3303 - val_loss: 0.3364\n",
      "Epoch 75/100\n",
      "363/363 [==============================] - 0s 785us/step - loss: 0.3147 - val_loss: 0.3262\n",
      "Epoch 76/100\n",
      "363/363 [==============================] - 0s 758us/step - loss: 0.3128 - val_loss: 0.3280\n",
      "Epoch 77/100\n",
      "363/363 [==============================] - 0s 791us/step - loss: 0.3117 - val_loss: 0.3258\n",
      "Epoch 78/100\n",
      "363/363 [==============================] - 0s 744us/step - loss: 0.3101 - val_loss: 0.3304\n",
      "Epoch 79/100\n",
      "363/363 [==============================] - 0s 751us/step - loss: 0.3088 - val_loss: 0.3253\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/100\n",
      "363/363 [==============================] - 0s 713us/step - loss: 0.3084 - val_loss: 0.3276\n",
      "Epoch 81/100\n",
      "363/363 [==============================] - 0s 731us/step - loss: 0.3083 - val_loss: 0.3312\n",
      "Epoch 82/100\n",
      "363/363 [==============================] - 0s 786us/step - loss: 0.3080 - val_loss: 0.3263\n",
      "Epoch 83/100\n",
      "363/363 [==============================] - 0s 961us/step - loss: 0.3069 - val_loss: 0.3206\n",
      "Epoch 84/100\n",
      "363/363 [==============================] - 0s 962us/step - loss: 0.3072 - val_loss: 0.3220\n",
      "Epoch 85/100\n",
      "363/363 [==============================] - 0s 904us/step - loss: 0.3086 - val_loss: 0.3208\n",
      "Epoch 86/100\n",
      "363/363 [==============================] - 0s 952us/step - loss: 0.3053 - val_loss: 0.3314\n",
      "Epoch 87/100\n",
      "363/363 [==============================] - 0s 816us/step - loss: 0.3061 - val_loss: 0.3243\n",
      "Epoch 88/100\n",
      "363/363 [==============================] - 0s 799us/step - loss: 0.3038 - val_loss: 0.3233\n",
      "Epoch 89/100\n",
      "363/363 [==============================] - 0s 735us/step - loss: 0.3051 - val_loss: 0.3248\n",
      "Epoch 90/100\n",
      "363/363 [==============================] - 0s 799us/step - loss: 0.3069 - val_loss: 0.3260\n",
      "Epoch 91/100\n",
      "363/363 [==============================] - 0s 792us/step - loss: 0.3057 - val_loss: 0.3306\n",
      "Epoch 92/100\n",
      "363/363 [==============================] - 0s 812us/step - loss: 0.3059 - val_loss: 0.3228\n",
      "Epoch 93/100\n",
      "363/363 [==============================] - 0s 809us/step - loss: 0.3038 - val_loss: 0.3203\n",
      "Epoch 94/100\n",
      "363/363 [==============================] - 0s 752us/step - loss: 0.3043 - val_loss: 0.3213\n",
      "Epoch 95/100\n",
      "363/363 [==============================] - 0s 747us/step - loss: 0.3037 - val_loss: 0.3270\n",
      "Epoch 96/100\n",
      "363/363 [==============================] - 0s 741us/step - loss: 0.3028 - val_loss: 0.3445\n",
      "Epoch 97/100\n",
      "363/363 [==============================] - 0s 734us/step - loss: 0.3023 - val_loss: 0.3205\n",
      "Epoch 98/100\n",
      "363/363 [==============================] - 0s 750us/step - loss: 0.3050 - val_loss: 0.3280\n",
      "Epoch 99/100\n",
      "363/363 [==============================] - 0s 729us/step - loss: 0.3084 - val_loss: 0.3294\n",
      "Epoch 100/100\n",
      "363/363 [==============================] - 0s 776us/step - loss: 0.3051 - val_loss: 0.3184\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3,\n",
       "                   estimator=<keras.wrappers.scikit_learn.KerasRegressor object at 0x00000289288D1A30>,\n",
       "                   param_distributions={'learning_rate': <scipy.stats._distn_infrastructure.rv_frozen object at 0x000002892735DE80>,\n",
       "                                        'n_hidden': [0, 1, 2, 3],\n",
       "                                        'n_neurons': array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34,\n",
       "       35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51,\n",
       "       52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68,\n",
       "       69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85,\n",
       "       86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99])})"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_distribs = {\n",
    "\"n_hidden\": [0, 1, 2, 3],\n",
    "\"n_neurons\": np.arange(1, 100),\n",
    "\"learning_rate\": reciprocal(3e-4, 3e-2),\n",
    "}\n",
    "rnd_search_cv = RandomizedSearchCV(keras_reg, param_distribs, n_iter=10, cv=3)\n",
    "rnd_search_cv.fit(X_train, y_train, epochs=100, validation_data=(X_valid, y_valid), callbacks=[keras.callbacks.EarlyStopping(patience=10)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.0032185140603213564, 'n_hidden': 2, 'n_neurons': 43}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnd_search_cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_search_cv.best_score_\n",
    "model = rnd_search_cv.best_estimator_.model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
